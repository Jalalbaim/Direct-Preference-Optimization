{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39146266",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83db0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation pour Colab (Python 3.12+)\n",
    "# Pin TRL to 0.26.0 for API compatibility\n",
    "!pip install --upgrade pip setuptools wheel -q\n",
    "!pip install transformers[torch] datasets trl==0.26.0 wandb accelerate -q\n",
    "\n",
    "# V√©rification\n",
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "print(\"‚úÖ Installation r√©ussie!\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"TRL: {trl.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193bc2a",
   "metadata": {},
   "source": [
    "## 0. Mount Google Drive (Optional - for Colab/Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive pour √©conomiser temps et quota\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    SAVE_BASE_PATH = '/content/drive/MyDrive/dpo_ppo_training'\n",
    "    os.makedirs(SAVE_BASE_PATH, exist_ok=True)\n",
    "    print(f\"‚úÖ Google Drive mont√©. Mod√®les sauvegard√©s sur: {SAVE_BASE_PATH}\")\n",
    "    USE_DRIVE = True\n",
    "except ImportError:\n",
    "    # Pas sur Colab\n",
    "    SAVE_BASE_PATH = './results'\n",
    "    USE_DRIVE = False\n",
    "    print(f\"‚ö†Ô∏è  Pas de Google Drive d√©tect√©. Stockage local: {SAVE_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c98bb",
   "metadata": {},
   "source": [
    "## 1. Configuration PPO\n",
    "\n",
    "**Param√®tres configurables :**\n",
    "- `USE_REWARD_MODEL` : False = classifier direct, True = reward model entra√Æn√©\n",
    "- `target_kl` : Divergence KL cible (3, 6, 9, 12)\n",
    "- `batch_size` : √Ä ajuster selon GPU T4\n",
    "- `max_new_tokens` : Tokens g√©n√©r√©s par prompt (24 par d√©faut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83576c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# =====================================\n",
    "# CONFIGURATION PRINCIPALE\n",
    "# =====================================\n",
    "\n",
    "# ‚≠ê PPO Mode: Trained Reward Model only\n",
    "USE_REWARD_MODEL = True  # Always use the trained reward model\n",
    "\n",
    "# PPO Hyperparameters\n",
    "TARGET_KL = 3.0  # Divergence KL cible\n",
    "BATCH_SIZE = 128  # √Ä ajuster selon votre GPU (T4: 64-128)\n",
    "MINI_BATCH_SIZE = 32  # batch_size / 4 g√©n√©ralement\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 1\n",
    "MAX_NEW_TOKENS = 24  # Tokens g√©n√©r√©s par prompt\n",
    "\n",
    "# Model paths\n",
    "SFT_MODEL_PATH = f\"{SAVE_BASE_PATH}/sft_model\"\n",
    "REWARD_MODEL_PATH = f\"{SAVE_BASE_PATH}/reward_model\"  # Trained reward model\n",
    "PPO_MODEL_PATH = f\"{SAVE_BASE_PATH}/ppo_model\"\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"PPO Configuration (Reward Model)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Target KL: {TARGET_KL}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Mini Batch Size: {MINI_BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Max New Tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"SFT Model: {SFT_MODEL_PATH}\")\n",
    "print(f\"Reward Model: {REWARD_MODEL_PATH}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd08ecd",
   "metadata": {},
   "source": [
    "## 2. Load Dataset (Prompts from DPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ea282",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 1: Chargement du dataset de prompts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load preference pairs from DPO dataset\n",
    "pairs_path = f\"{SAVE_BASE_PATH}/datasets/preference_pairs.json\"\n",
    "\n",
    "if not os.path.exists(pairs_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Dataset introuvable: {pairs_path}\\n\"\n",
    "        \"Veuillez d'abord ex√©cuter le notebook de g√©n√©ration des paires de pr√©f√©rences.\"\n",
    "    )\n",
    "\n",
    "# Load prompts\n",
    "print(f\"\\nüì• Chargement des prompts depuis: {pairs_path}\")\n",
    "with open(pairs_path, 'r', encoding='utf-8') as f:\n",
    "    preference_pairs = json.load(f)\n",
    "\n",
    "# Extract unique prompts and their labels\n",
    "prompts = []\n",
    "prompt_labels = {}  # prompt -> sentiment_label (1=positive, 0=negative)\n",
    "\n",
    "for pair in preference_pairs:\n",
    "    prompt = pair[\"prompt\"]\n",
    "    if prompt not in prompts:\n",
    "        prompts.append(prompt)\n",
    "        # Try to infer label from chosen text (positive sentiment = 1)\n",
    "        # For IMDB, we assume \"chosen\" is positive (sentiment=1)\n",
    "        prompt_labels[prompt] = 1  # Ground truth: chosen is positive\n",
    "\n",
    "print(f\"‚úÖ {len(prompts)} prompts uniques charg√©s\")\n",
    "print(f\"\\nExemples de prompts:\")\n",
    "for i in range(min(3, len(prompts))):\n",
    "    print(f\"  {i+1}. {prompts[i][:50]}...\")\n",
    "\n",
    "# Create dataset\n",
    "ppo_dataset = Dataset.from_dict({\"query\": prompts})\n",
    "print(f\"\\n‚úÖ Dataset PPO cr√©√© avec {len(ppo_dataset)} prompts\")\n",
    "print(f\"{'='*80}\")\n",
    "if USE_GT_REWARD:\n",
    "    print(f\"‚≠ê MODE: PPO-GT (Ground Truth Oracle)\")\n",
    "else:\n",
    "    reward_type = \"Learned Reward Model\" if USE_REWARD_MODEL else \"Siebert Classifier\"\n",
    "    print(f\"üìö MODE: PPO Standard ({reward_type})\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05fd03a",
   "metadata": {},
   "source": [
    "## 3. Load Models (Policy, Reference, Reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 2: Chargement des mod√®les\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nüì• Chargement du tokenizer depuis: {SFT_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(\"‚úÖ Tokenizer charg√©\")\n",
    "\n",
    "# Load policy model (with value head for PPO)\n",
    "print(f\"\\nüì• Chargement du policy model (SFT + value head)...\")\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    policy_model = policy_model.to(\"cuda\")\n",
    "    print(f\"‚úÖ Policy model charg√© sur GPU ({policy_model.pretrained_model.num_parameters() / 1e9:.2f}B params)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Policy model charg√© sur CPU\")\n",
    "\n",
    "# Load reference model (frozen SFT)\n",
    "print(f\"\\nüì• Chargement du reference model (SFT frozen)...\")\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    ref_model = ref_model.to(\"cuda\")\n",
    "    print(f\"‚úÖ Reference model charg√© sur GPU (frozen)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Reference model charg√© sur CPU\")\n",
    "\n",
    "# Freeze reference model\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "ref_model.eval()\n",
    "\n",
    "# Ensure generation_config exists and is attached to wrappers\n",
    "for m in [policy_model, ref_model]:\n",
    "    base = m.pretrained_model\n",
    "    try:\n",
    "        gen_cfg = base.generation_config\n",
    "    except AttributeError:\n",
    "        gen_cfg = None\n",
    "    if gen_cfg is None:\n",
    "        gen_cfg = GenerationConfig.from_model_config(base.config)\n",
    "    if gen_cfg.pad_token_id is None:\n",
    "        gen_cfg.pad_token_id = tokenizer.pad_token_id\n",
    "    base.generation_config = gen_cfg\n",
    "    m.generation_config = gen_cfg\n",
    "\n",
    "# Load reward model (entra√Æn√©)\n",
    "print(f\"\\nüì• Chargement du reward model (entra√Æn√©)...\")\n",
    "if not os.path.exists(REWARD_MODEL_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Reward model introuvable: {REWARD_MODEL_PATH}\\n\"\n",
    "        \"Veuillez d'abord entra√Æner le reward model dans Train_Reward_Model_NVIDIA.ipynb\"\n",
    "    )\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    REWARD_MODEL_PATH,\n",
    "    num_labels=2,  # Binaire: positive (1) / negative (0)\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_PATH)\n",
    "\n",
    "if torch.cuda.is_available() and reward_model.device.type != 'cuda':\n",
    "    reward_model = reward_model.to(\"cuda\")\n",
    "\n",
    "reward_model.eval()\n",
    "for param in reward_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"‚úÖ Reward model charg√© sur GPU (frozen)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Tous les mod√®les charg√©s avec succ√®s\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad258f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüì• Chargement du value model (critic)...\")\n",
    "value_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    value_model = value_model.to(\"cuda\")\n",
    "    print(\"‚úÖ Value model charg√© sur GPU\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Value model charg√© sur CPU\")\n",
    "\n",
    "# Attach generation_config to value model wrapper\n",
    "from transformers import GenerationConfig\n",
    "base = value_model.pretrained_model\n",
    "try:\n",
    "    gen_cfg = base.generation_config\n",
    "except AttributeError:\n",
    "    gen_cfg = None\n",
    "if gen_cfg is None:\n",
    "    gen_cfg = GenerationConfig.from_model_config(base.config)\n",
    "if gen_cfg.pad_token_id is None:\n",
    "    gen_cfg.pad_token_id = tokenizer.pad_token_id\n",
    "base.generation_config = gen_cfg\n",
    "value_model.generation_config = gen_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09bc10",
   "metadata": {},
   "source": [
    "## 4. Initialize W&B Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc32ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B\n",
    "wandb.login()\n",
    "\n",
    "# Initialize W&B run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"ppo_imdb_reward_kl{TARGET_KL}_{timestamp}\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"ppo\",\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"model\": \"gpt2-large\",\n",
    "        \"dataset\": \"imdb_prompts\",\n",
    "        \"num_prompts\": len(ppo_dataset),\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"mini_batch_size\": MINI_BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"target_kl\": TARGET_KL,\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"reward_model_type\": \"trained\",\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ W&B initialized: {run_name}\")\n",
    "print(f\"   Project: ppo\")\n",
    "print(f\"   Target KL: {TARGET_KL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e5fb1",
   "metadata": {},
   "source": [
    "## 5. Configure PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3992982",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 3: Configuration du PPO Trainer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# PPO Configuration\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    mini_batch_size=MINI_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    kl_coef=TARGET_KL,  # Pond√©ration du terme KL\n",
    "    num_ppo_epochs=4,\n",
    "    seed=42,\n",
    "    report_to=\"wandb\",  # TRL 0.26.x utilise TrainingArguments.report_to\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "# Initialize PPO Trainer\n",
    "print(f\"\\nüöÄ Initialisation du PPO Trainer...\")\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ppo_dataset,\n",
    "    reward_model=reward_model,\n",
    "    value_model=value_model,\n",
    " )\n",
    "\n",
    "print(f\"‚úÖ PPO Trainer initialis√©\")\n",
    "print(f\"\\nüìä Configuration:\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Mini batch size: {MINI_BATCH_SIZE}\")\n",
    "print(f\"   - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   - KL coef: {TARGET_KL}\")\n",
    "print(f\"   - PPO epochs per batch: 4\")\n",
    "print(f\"   - Max new tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"   - Total prompts: {len(ppo_dataset)}\")\n",
    "print(f\"   - Estimated batches: {len(ppo_dataset) // BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17aae4",
   "metadata": {},
   "source": [
    "## 6. Define Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rewards(texts, queries=None):\n",
    "    \"\"\"\n",
    "    Compute rewards for generated texts.\n",
    "    \n",
    "    Two modes:\n",
    "    - USE_GT_REWARD=True: PPO-GT Oracle - use ground truth label (1.0 for all samples)\n",
    "    - USE_GT_REWARD=False: PPO Standard - use learned reward (classifier or model)\n",
    "    \n",
    "    Args:\n",
    "        texts: List of generated text strings (prompt + response)\n",
    "        queries: List of original prompts (for GT mode)\n",
    "        \n",
    "    Returns:\n",
    "        List of reward scores (floats)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    if USE_GT_REWARD:\n",
    "        # ‚≠ê PPO-GT MODE: Oracle with ground truth rewards\n",
    "        # In IMDB sentiment task: positive sentiment = 1.0\n",
    "        # For PPO-GT, all samples get reward = 1.0 (oracle knows they're trying to be positive)\n",
    "        # More realistically: reward based on how positive the text is from the IMDB perspective\n",
    "        \n",
    "        # Simplified: use ground truth that all prompts should lead to positive sentiment\n",
    "        rewards = [1.0] * len(texts)\n",
    "        \n",
    "        print(f\"  [PPO-GT] Using ground truth rewards: {len(rewards)} samples with reward=1.0\")\n",
    "    else:\n",
    "        # üìö PPO STANDARD MODE: Learned reward from classifier or model\n",
    "        for text in texts:\n",
    "            # Truncate to 512 tokens for classifier\n",
    "            truncated = text[:512]\n",
    "            \n",
    "            if USE_REWARD_MODEL:\n",
    "                # Option A: Trained reward model\n",
    "                inputs = reward_tokenizer(\n",
    "                    truncated,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    padding=True\n",
    "                ).to(reward_model.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = reward_model(**inputs)\n",
    "                    reward = outputs.logits[0, 0].item()  # Scalar reward\n",
    "            else:\n",
    "                # Option B: Direct classifier (siebert)\n",
    "                inputs = reward_tokenizer(\n",
    "                    truncated,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    padding=True\n",
    "                ).to(reward_model.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = reward_model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    # POSITIVE is label 1, reward = positive probability\n",
    "                    reward = probs[0, 1].item()\n",
    "            \n",
    "            rewards.append(reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Test reward function\n",
    "print(\"Testing reward function...\")\n",
    "test_texts = [\n",
    "    \"This movie is amazing and wonderful!\",\n",
    "    \"This movie is terrible and boring.\"\n",
    "]\n",
    "test_rewards = compute_rewards(test_texts)\n",
    "print(f\"\\nTest rewards:\")\n",
    "for text, reward in zip(test_texts, test_rewards):\n",
    "    print(f\"  Text: {text[:50]}...\")\n",
    "    print(f\"  Reward: {reward:.4f}\\n\")\n",
    "print(\"‚úÖ Reward function working correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdaaf64",
   "metadata": {},
   "source": [
    "## 7. PPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 4: Entra√Ænement PPO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Checkpointing config\n",
    "RESUME_FROM_CHECKPOINT = True\n",
    "CHECKPOINT_DIR = Path(PPO_MODEL_PATH) / \"checkpoints\"\n",
    "CHECKPOINT_SAVE_STEPS = 200  # save optimizer/scheduler/model states every N steps\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Try to resume from latest checkpoint\n",
    "latest_ckpt = None\n",
    "if RESUME_FROM_CHECKPOINT:\n",
    "    ckpts = sorted(\n",
    "        CHECKPOINT_DIR.glob(\"step_*\"),\n",
    "        key=lambda p: int(p.name.split(\"_\")[-1]) if p.name.split(\"_\")[-1].isdigit() else -1,\n",
    "    )\n",
    "    if ckpts:\n",
    "        latest_ckpt = str(ckpts[-1])\n",
    "        print(f\"üîÑ Reprise depuis le checkpoint: {latest_ckpt}\")\n",
    "        ppo_trainer.accelerator.load_state(latest_ckpt)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Aucun checkpoint trouv√©, entra√Ænement from scratch\")\n",
    "\n",
    "# Generation kwargs\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "}\n",
    "\n",
    "print(f\"\\nüöÄ D√©marrage de l'entra√Ænement PPO...\")\n",
    "print(f\"   - {len(ppo_dataset)} prompts\")\n",
    "print(f\"   - {len(ppo_dataset) // BATCH_SIZE} batches\")\n",
    "print(f\"   - {NUM_EPOCHS} epoch(s)\\n\")\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(ppo_trainer.dataloader, desc=f\"Epoch {epoch+1}\")):\n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "        \n",
    "        # Generate responses\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            query_tensors,\n",
    "            return_prompt=False,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        # Decode responses\n",
    "        batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "        \n",
    "        # Compute rewards\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        rewards = compute_rewards(texts)\n",
    "        rewards = [torch.tensor(r) for r in rewards]\n",
    "        \n",
    "        # Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        \n",
    "        # Log statistics\n",
    "        ppo_trainer.log_stats(\n",
    "            stats,\n",
    "            batch,\n",
    "            rewards,\n",
    "            columns_to_log=[\"query\", \"response\"]\n",
    "        )\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "        # Print progress every 10 batches\n",
    "        if global_step % 10 == 0:\n",
    "            mean_reward = np.mean([r.item() for r in rewards])\n",
    "            mean_kl = stats.get(\"objective/kl\", 0.0)\n",
    "            print(f\"\\nBatch {global_step}: Mean Reward = {mean_reward:.4f}, KL = {mean_kl:.4f}\")\n",
    "        \n",
    "        # Periodic checkpoint save\n",
    "        if global_step % CHECKPOINT_SAVE_STEPS == 0:\n",
    "            ckpt_path = CHECKPOINT_DIR / f\"step_{global_step}\"\n",
    "            ppo_trainer.accelerator.save_state(str(ckpt_path))\n",
    "            print(f\"üíæ Checkpoint sauvegard√©: {ckpt_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Entra√Ænement PPO termin√©!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72e1eb",
   "metadata": {},
   "source": [
    "## 8. Save PPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bf74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 5: Sauvegarde du mod√®le PPO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save model\n",
    "os.makedirs(PPO_MODEL_PATH, exist_ok=True)\n",
    "print(f\"\\nüíæ Sauvegarde du mod√®le PPO dans: {PPO_MODEL_PATH}\")\n",
    "\n",
    "# Save the pretrained model (without value head)\n",
    "policy_model.save_pretrained(PPO_MODEL_PATH)\n",
    "tokenizer.save_pretrained(PPO_MODEL_PATH)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le PPO sauvegard√©!\")\n",
    "print(f\"\\nüìÅ Fichiers cr√©√©s:\")\n",
    "print(f\"   - {PPO_MODEL_PATH}/pytorch_model.bin\")\n",
    "print(f\"   - {PPO_MODEL_PATH}/config.json\")\n",
    "print(f\"   - {PPO_MODEL_PATH}/tokenizer.json\")\n",
    "\n",
    "# Close W&B run\n",
    "wandb.finish()\n",
    "print(f\"\\n‚úÖ W&B run closed\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ PPO TRAINING COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nüéØ Prochaines √©tapes:\")\n",
    "print(f\"   1. √âvaluer le mod√®le PPO sur le test set\")\n",
    "print(f\"   2. Comparer avec SFT et DPO\")\n",
    "print(f\"   3. G√©n√©rer la courbe reward-KL (Figure 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea4663f",
   "metadata": {},
   "source": [
    "## 9. Test PPO Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da45190",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Test du mod√®le PPO sur quelques exemples\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load PPO model for testing\n",
    "test_model = AutoModelForCausalLM.from_pretrained(PPO_MODEL_PATH)\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(PPO_MODEL_PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    test_model = test_model.to(\"cuda\")\n",
    "\n",
    "test_model.eval()\n",
    "\n",
    "# Test on 5 random prompts\n",
    "import random\n",
    "random.seed(42)\n",
    "test_prompts = random.sample(prompts, 5)\n",
    "\n",
    "print(f\"\\nG√©n√©ration sur 5 prompts al√©atoires:\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = test_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=test_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    continuation = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"Generated: {continuation}\")\n",
    "    \n",
    "    # Compute reward\n",
    "    reward = compute_rewards([generated_text])[0]\n",
    "    print(f\"Reward: {reward:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚úÖ Test termin√©!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
