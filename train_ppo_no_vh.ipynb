{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eef7ce4",
   "metadata": {},
   "source": [
    "## 1. Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changer de r√©pertoire vers le projet (si n√©cessaire)\n",
    "# %cd Direct-Preference-Optimization/\n",
    "\n",
    "import os\n",
    "print(f\"üìÇ R√©pertoire actuel: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installation silencieuse des packages\n",
    "!pip install torch transformers datasets accelerate bitsandbytes sentencepiece protobuf tqdm pyyaml scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e51efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Ajouter le r√©pertoire racine au path\n",
    "ROOT = os.path.abspath('.')\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.append(ROOT)\n",
    "\n",
    "from src.core.models import load_models\n",
    "from src.core.data import PromptDataset, prompt_collate_fn\n",
    "from src.ppo.ppo_trainer_no_vh import PPOTrainerNoValueHead\n",
    "from src.core.utils import load_yaml_config\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c2206",
   "metadata": {},
   "source": [
    "## 2. V√©rification des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afb9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_path = \"data/processed/sentiment/prompts.jsonl\"\n",
    "\n",
    "if not os.path.exists(prompts_path):\n",
    "    print(\"‚ö†Ô∏è  Fichier prompts.jsonl non trouv√©!\")\n",
    "    print(\"Ex√©cution de prepare_prompts.py...\")\n",
    "    !python scripts/prepare_prompts.py\n",
    "else:\n",
    "    with open(prompts_path, 'r') as f:\n",
    "        num_prompts = sum(1 for line in f if line.strip())\n",
    "    print(f\"‚úÖ {num_prompts} prompts trouv√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c367ec",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dadf261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger la config NO VALUE HEAD\n",
    "config_path = \"configs/ppo_sentiment_no_vh.yaml\"\n",
    "config = load_yaml_config(config_path)\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration PPO (NO VALUE HEAD):\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(\"\\nüìä Param√®tres PPO:\")\n",
    "print(f\"  Clip epsilon: {config['ppo']['clip_epsilon']}\")\n",
    "print(f\"  Entropy coef: {config['ppo']['entropy_coef']}\")\n",
    "print(f\"  ‚ùå PAS de value_coef (pas de value head)\")\n",
    "print(f\"  Target KL: {config['ppo']['target_kl']}\")\n",
    "print(f\"  PPO epochs: {config['ppo']['num_ppo_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7f521",
   "metadata": {},
   "source": [
    "## 4. Chargement des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdd03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üì± Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config[\"model\"][\"name\"]\n",
    "dtype = config[\"model\"][\"dtype\"]\n",
    "\n",
    "mb = load_models(model_name, dtype=dtype, device=device)\n",
    "tokenizer = mb.tokenizer\n",
    "\n",
    "print(f\"‚úÖ Mod√®les charg√©s: {model_name}\")\n",
    "print(f\"   Policy model: {mb.policy_model.num_parameters():,} param√®tres\")\n",
    "print(f\"   Device: {mb.device}\")\n",
    "print(f\"   ‚ùå PAS de value head - √âconomie de param√®tres!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138276b",
   "metadata": {},
   "source": [
    "## 5. Pr√©paration du DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf69fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataset = PromptDataset(config[\"data\"][\"prompt_path\"])\n",
    "max_prompt_length = config[\"data\"][\"max_prompt_length\"]\n",
    "\n",
    "print(f\"Dataset: {len(prompt_dataset)} prompts\")\n",
    "\n",
    "def collate(batch):\n",
    "    return prompt_collate_fn(\n",
    "        batch,\n",
    "        tokenizer=tokenizer,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "    )\n",
    "\n",
    "prompt_loader = DataLoader(\n",
    "    prompt_dataset,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoader cr√©√©: {len(prompt_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c30893",
   "metadata": {},
   "source": [
    "## 6. Initialisation du PPO Trainer (No VH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fdb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è  Initialisation du PPO Trainer (NO VALUE HEAD)...\")\n",
    "\n",
    "trainer = PPOTrainerNoValueHead(\n",
    "    model_bundle=mb,\n",
    "    prompt_loader=prompt_loader,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Trainer initialis√©\")\n",
    "print(f\"   Reward model: {config['reward_model']['name']}\")\n",
    "print(f\"   Save dir: {config['logging']['save_dir']}\")\n",
    "print(f\"   ‚úì Mode: PPO sans Value Head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8c143",
   "metadata": {},
   "source": [
    "## 7. Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e9652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT PPO (NO VALUE HEAD)\")\n",
    "print(f\"   Exp√©rience: {config['experiment_name']}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   Total batches: {len(prompt_loader)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚úÖ Entra√Ænement termin√©!\")\n",
    "    print(f\"   Temps total: {elapsed/60:.2f} minutes\")\n",
    "    print(f\"   Temps moyen par epoch: {elapsed/config['training']['num_epochs']/60:.2f} minutes\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è  Entra√Ænement interrompu par l'utilisateur\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚ùå ERREUR: {type(e).__name__}\")\n",
    "    print(f\"   Message: {e}\")\n",
    "    print(\"=\"*60)\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a27723",
   "metadata": {},
   "source": [
    "## 8. V√©rification des Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2097d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "save_dir = config['logging']['save_dir']\n",
    "checkpoints = glob.glob(os.path.join(save_dir, \"*.pt\"))\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"‚úÖ {len(checkpoints)} checkpoint(s) sauvegard√©(s):\")\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        size_mb = os.path.getsize(ckpt) / (1024**2)\n",
    "        print(f\"   üìÅ {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Aucun checkpoint trouv√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3362c",
   "metadata": {},
   "source": [
    "## 9. Test de G√©n√©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12de615",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"The movie was\",\n",
    "    \"I think this product is\",\n",
    "    \"The customer service was\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Test de g√©n√©ration (PPO No VH):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mb.policy_model.eval()\n",
    "with torch.no_grad():\n",
    "    for prompt in test_prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = mb.policy_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\nüí¨ Prompt: {prompt}\")\n",
    "        print(f\"‚ú® Response: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778db1c",
   "metadata": {},
   "source": [
    "## üìä Comparaison PPO vs PPO No VH\n",
    "\n",
    "| Aspect | PPO (avec VH) | PPO (sans VH) |\n",
    "|--------|---------------|---------------|\n",
    "| **Param√®tres** | +1-5M (value head) | Aucun param√®tre suppl√©mentaire |\n",
    "| **VRAM** | ~17-18 GB | ~15-16 GB |\n",
    "| **Complexit√©** | Plus complexe (value loss) | Plus simple |\n",
    "| **Variance** | Moins de variance | Peut avoir plus de variance |\n",
    "| **Stabilit√©** | G√©n√©ralement plus stable | D√©pend du reward model |\n",
    "| **Vitesse** | L√©g√®rement plus rapide | Appels suppl√©mentaires au RM |\n",
    "\n",
    "## üí° Quand utiliser quelle version?\n",
    "\n",
    "**PPO avec Value Head:**\n",
    "- Probl√®mes complexes avec trajectoires longues\n",
    "- Besoin de variance minimale\n",
    "- Reward model co√ªteux en calcul\n",
    "\n",
    "**PPO sans Value Head:**\n",
    "- Contraintes de VRAM\n",
    "- Reward model rapide et fiable\n",
    "- Besoin de simplicit√©\n",
    "- Exp√©rimentation rapide"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
