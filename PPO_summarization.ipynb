{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c107861e",
   "metadata": {},
   "source": [
    "This code is based on the paper provided by Stiennon et al. that shows how to use PPO to train a LM using human feedback for a summarization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f18f63",
   "metadata": {},
   "source": [
    "# IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eda0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea677f96",
   "metadata": {},
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_beta = 0.5\n",
    "batch_size = 10\n",
    "epoch_nb = 5\n",
    "clip_range = 0.2\n",
    "return_sequence_nb = 4\n",
    "beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype  = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "def set_seed(seed = 456):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print(\"Device:\", device, \"| dtype:\", dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246f089",
   "metadata": {},
   "source": [
    "# LOADING SFT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CarperAI/summarize_from_feedback_sft\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "policy_model_ref = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "policy_model.to(device)\n",
    "policy_model_ref.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_model_name = \"gpt3\"\n",
    "\n",
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, base_model_name):\n",
    "        super().__init__()\n",
    "        self.base = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "        # Tête de valeur : transforme le hidden_state en un score scalaire\n",
    "        self.v_head = nn.Linear(self.base.config.n_embd, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        # On utilise le dernier état caché\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        values = self.v_head(last_hidden_state).squeeze(-1)\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2451aa",
   "metadata": {},
   "source": [
    "# LOADING REDDIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"openai/summarize_from_feedback\", \"comparisons\")\n",
    "\n",
    "def build_prompt(example):\n",
    "    return f\"Summarize the following text:\\n{example['text']}\\nSummary:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dpo(dataset):\n",
    "    return dataset.map(\n",
    "        lambda x: {\n",
    "            \"prompt\": x[\"info\"][\"post\"].strip(),\n",
    "            \"chosen\": x[\"summaries\"][x[\"choice\"]][\"text\"].strip(),\n",
    "            \"rejected\": x[\"summaries\"][1 - x[\"choice\"]][\"text\"].strip(),\n",
    "        },\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "ppo_ds = format_dpo(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687b523",
   "metadata": {},
   "source": [
    "# TRAINING REWARD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4477d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_lm):\n",
    "        super().__init__()\n",
    "        self.lm = base_lm\n",
    "        self.reward_head = nn.Linear(base_lm.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.lm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        output_hidden_states=True\n",
    "        )\n",
    "        last_hidden = outputs.hidden_states[-1][:, -1]\n",
    "        return self.reward_head(last_hidden).squeeze(-1)\n",
    "\n",
    "\n",
    "reward_model = RewardModel(\n",
    "    AutoModelForCausalLM.from_pretrained(model_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the reward model\n",
    "optimizer = AdamW(reward_model.parameters(), lr=1e-5)\n",
    "\n",
    "for batch in ppo_ds.select(range(batch_size)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs_w = tokenizer(batch['prompt'] + batch['chosen'], return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs_l = tokenizer(batch['prompt'] + batch['rejected'], return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "\n",
    "    r_w = reward_model(**inputs_w)\n",
    "    r_l = reward_model(**inputs_l)\n",
    "\n",
    "    loss = -torch.log(torch.sigmoid(r_w - r_l)).mean() #Based on paper page 6\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b02c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_model = CriticModel(critic_model_name)\n",
    "critic_model.base.load_state_dict(reward_model.lm.state_dict())\n",
    "critic_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d5366",
   "metadata": {},
   "source": [
    "# TRAINING PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(logits, labels):\n",
    "    \"\"\"Calcule les log-probabilités des tokens générés.\"\"\"\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    # On décale pour aligner les logits avec les labels générés\n",
    "    log_probs = log_probs[:, :-1, :]\n",
    "    labels = labels[:, 1:]\n",
    "    return torch.gather(log_probs, -1, labels.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(values, rewards, mask, gamma=1.0, lam=0.95):\n",
    "    \"\"\"Generalized Advantage Estimation conforme au papier.\"\"\"\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    last_gae = 0\n",
    "    for t in reversed(range(rewards.size(1))):\n",
    "        # Pour la summarization, la récompense est souvent clairsemée (à la fin)\n",
    "        next_value = values[:, t + 1] if t + 1 < rewards.size(1) else 0\n",
    "        delta = rewards[:, t] + gamma * next_value - values[:, t]\n",
    "        advantages[:, t] = last_gae = delta + gamma * lam * last_gae\n",
    "    returns = advantages + values\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dba72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_mask(full_seq, prompt_len):\n",
    "    mask = torch.ones_like(full_seq)\n",
    "    mask[:, :prompt_len] = 0\n",
    "    mask[full_seq == tokenizer.pad_token_id] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model.eval()\n",
    "\n",
    "\n",
    "for batch in ppo_ds.select(range(batch)):\n",
    "    prompt = build_prompt(batch)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
    "    prompt_len = inputs['input_ids'].size(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summaries = policy_model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_new_tokens=64\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=return_sequence_nb\n",
    "        )\n",
    "\n",
    "        #Preparing for scoring\n",
    "        masks = (summaries != tokenizer.pad_token_id).long()\n",
    "        rm_scores = reward_model(summaries, attention_mask=masks).detach()\n",
    "        best_idx = rm_scores.argmax()\n",
    "        summary = summaries[best_idx].unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "        mask = (summary != tokenizer.pad_token_id).long()\n",
    "\n",
    "        response_mask = get_response_mask(summary, prompt_len)\n",
    "        response_mask_shifted = response_mask[:, 1:]\n",
    "\n",
    "        old_logits = policy_model(summary,attention_mask=mask,).logits\n",
    "        ref_logits = policy_model_ref(summary,attention_mask=mask,).logits\n",
    "\n",
    "        #Computing the log prob\n",
    "        old_logprob = get_log_probs(old_logits, summary).detach()\n",
    "        ref_logits = get_log_probs(ref_logits, summary)\n",
    "\n",
    "        #Reward computation\n",
    "        rm_score = reward_model(summary, attention_mask=mask).detach()\n",
    "\n",
    "        #Simulating the human feedback policy\n",
    "        kl_dist = old_logprob - ref_logits\n",
    "        rewards = -kl_beta * kl_dist + rm_score\n",
    "        last_idx = response_mask_shifted.sum(dim=1) - 1\n",
    "        rewards[0, last_idx] += rm_score\n",
    "\n",
    "        #Computing the advantage\n",
    "        values = critic_model(summary, attention_mask=mask)[:,:-1]\n",
    "        advantages, returns = compute_gae(values, rewards, mask[:,:-1])\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        advantages = advantages.detach()\n",
    "        returns = returns.detach()\n",
    "\n",
    "    #Actual PPO update\n",
    "    for _ in range(epoch_nb):\n",
    "        logits = policy_model(summary, attention_mask=mask).logits\n",
    "        logprob = get_log_probs(logits, summary)\n",
    "        value = critic_model(summary, attention_mask=mask)[:,:-1]\n",
    "\n",
    "        #Clipping ratio\n",
    "        ratio = torch.exp(logprob - old_logprob)\n",
    "\n",
    "        #Policy loss\n",
    "        clip1 = ratio * advantages\n",
    "        clip2 = torch.clamp(ratio, 1 - clip_range, 1 + clip_range) * advantages\n",
    "        policy_loss = -(torch.min(clip1, clip2) * response_mask_shifted).sum() / response_mask_shifted.sum()\n",
    "        #Now going back to the critic\n",
    "        value_loss = (F.mse_loss(value, returns, reduction='none') * response_mask_shifted).sum() / response_mask_shifted.sum()\n",
    "\n",
    "        loss = policy_loss + beta * value_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item():.4f} | RM Score: {rm_score.mean().item():.4f} | KL: {kl_div.mean().item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
