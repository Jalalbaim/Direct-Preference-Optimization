{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c107861e",
   "metadata": {},
   "source": [
    "This code is based on the paper provided by Stiennon et al. that shows how to use PPO to train a LM using human feedback for a summarization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f18f63",
   "metadata": {},
   "source": [
    "# IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eda0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246f089",
   "metadata": {},
   "source": [
    "# LOADING SFT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "policy_model_ref = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2451aa",
   "metadata": {},
   "source": [
    "# LOADING REDDIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"openai/summarize_from_feedback\", \"comparisons\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
