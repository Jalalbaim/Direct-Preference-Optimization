{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c107861e",
   "metadata": {},
   "source": [
    "This code is based on the paper provided by Stiennon et al. that shows how to use PPO to train a LM using human feedback for a summarization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f18f63",
   "metadata": {},
   "source": [
    "# IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eda0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea677f96",
   "metadata": {},
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_beta = 0.5\n",
    "batch_size = 10\n",
    "epoch_nb = 5\n",
    "clip_range = 0.2\n",
    "return_sequence_nb = 4\n",
    "beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype  = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "def set_seed(seed = 456):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print(\"Device:\", device, \"| dtype:\", dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246f089",
   "metadata": {},
   "source": [
    "# LOADING SFT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CarperAI/summarize_from_feedback_sft\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "policy_model_ref = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "policy_model.to(device)\n",
    "policy_model_ref.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_model_name = \"gpt3\"\n",
    "\n",
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, base_model_name):\n",
    "        super().__init__()\n",
    "        self.base = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "        # Tête de valeur : transforme le hidden_state en un score scalaire\n",
    "        self.v_head = nn.Linear(self.base.config.n_embd, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        # On utilise le dernier état caché\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        values = self.v_head(last_hidden_state).squeeze(-1)\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2451aa",
   "metadata": {},
   "source": [
    "# LOADING REDDIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"openai/summarize_from_feedback\", \"comparisons\")\n",
    "\n",
    "def build_prompt(example):\n",
    "    return f\"Summarize the following text:\\n{example['text']}\\nSummary:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dpo(dataset):\n",
    "    return dataset.map(\n",
    "        lambda x: {\n",
    "            \"prompt\": x[\"info\"][\"post\"].strip(),\n",
    "            \"chosen\": x[\"summaries\"][x[\"choice\"]][\"text\"].strip(),\n",
    "            \"rejected\": x[\"summaries\"][1 - x[\"choice\"]][\"text\"].strip(),\n",
    "        },\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "ppo_ds = format_dpo(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687b523",
   "metadata": {},
   "source": [
    "# TRAINING REWARD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4477d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_lm):\n",
    "        super().__init__()\n",
    "        self.lm = base_lm\n",
    "        self.reward_head = nn.Linear(base_lm.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.lm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        output_hidden_states=True\n",
    "        )\n",
    "        last_hidden = outputs.hidden_states[-1][:, -1]\n",
    "        return self.reward_head(last_hidden).squeeze(-1)\n",
    "\n",
    "\n",
    "reward_model = RewardModel(\n",
    "    AutoModelForCausalLM.from_pretrained(model_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the reward model\n",
    "reward_optimizer = AdamW(reward_model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "for batch in ppo_ds.select(range(batch_size)):\n",
    "    reward_optimizer.zero_grad()\n",
    "\n",
    "    inputs_w = tokenizer(batch['prompt'] + batch['chosen'], return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs_l = tokenizer(batch['prompt'] + batch['rejected'], return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "\n",
    "    r_w = reward_model(**inputs_w)\n",
    "    r_l = reward_model(**inputs_l)\n",
    "\n",
    "    loss = -torch.log(torch.sigmoid(r_w - r_l)).mean() #Based on paper page 6\n",
    "    loss.backward()\n",
    "    reward_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b02c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_model = CriticModel(critic_model_name)\n",
    "critic_model.base.load_state_dict(reward_model.lm.state_dict())\n",
    "critic_model.to(device)\n",
    "critic_optimizer = torch.optim.AdamW(critic_model.parameters(), lr=1e-5)\n",
    "policy_optimizer = torch.optim.AdamW(critic_model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d5366",
   "metadata": {},
   "source": [
    "# TRAINING PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(logits, labels):\n",
    "    \"\"\"Calcule les log-probabilités des tokens générés.\"\"\"\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    # On décale pour aligner les logits avec les labels générés\n",
    "    log_probs = log_probs[:, :-1, :]\n",
    "    labels = labels[:, 1:]\n",
    "    return torch.gather(log_probs, -1, labels.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(values, rewards, mask, gamma=1.0, lam=0.95):\n",
    "    \"\"\"Generalized Advantage Estimation conforme au papier.\"\"\"\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    last_gae = 0\n",
    "    for t in reversed(range(rewards.size(1))):\n",
    "        # Pour la summarization, la récompense est souvent clairsemée (à la fin)\n",
    "        next_value = values[:, t + 1] if t + 1 < rewards.size(1) else 0\n",
    "        delta = rewards[:, t] + gamma * next_value - values[:, t]\n",
    "        advantages[:, t] = last_gae = delta + gamma * lam * last_gae\n",
    "    returns = advantages + values\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dba72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_mask(full_seq, prompt_len):\n",
    "    mask = torch.ones_like(full_seq)\n",
    "    mask[:, :prompt_len] = 0\n",
    "    mask[full_seq == tokenizer.pad_token_id] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b293ac2",
   "metadata": {},
   "source": [
    "# WIN RATE EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "judge_tokenizer = AutoTokenizer.from_pretrained(judge_name)\n",
    "judge_model = AutoModelForCausalLM.from_pretrained(\n",
    "    judge_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an evaluator.\n",
    "\n",
    "    Given the post and two summaries, choose which summary is better.\n",
    "\n",
    "    Reply with exactly ONE character:\n",
    "    A or B\n",
    "\n",
    "    Do not explain.\n",
    "    Do not output anything else.\n",
    "\n",
    "    Post:\n",
    "    {post}\n",
    "\n",
    "    Summary A:\n",
    "    {summary_a}\n",
    "\n",
    "    Summary B:\n",
    "    {summary_b}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09951802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def evaluate_periodic_win_rate(\n",
    "    policy_model: torch.nn.Module,\n",
    "    ref_model: torch.nn.Module,\n",
    "    judge_model: torch.nn.Module,\n",
    "    judge_tokenizer: AutoTokenizer,\n",
    "    dataset: list,\n",
    "    prompt_template: str,\n",
    "    device: str = \"cuda\",\n",
    "    max_new_tokens: int = 64,\n",
    "    temperature: float = 0.25,\n",
    "    top_p: float = 0.9,\n",
    "    subset_size: int = 100,\n",
    "    verbose: bool = True,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Évalue le win rate du modèle de politique (DPO/PPO) par rapport au modèle de référence\n",
    "    en utilisant un modèle juge pour comparer les réponses.\n",
    "\n",
    "    Args:\n",
    "        policy_model: Modèle de politique (DPO/PPO) à évaluer.\n",
    "        ref_model: Modèle de référence (ex: SFT).\n",
    "        judge_model: Modèle utilisé comme juge (ex: Qwen, GPT-4).\n",
    "        judge_tokenizer: Tokenizer pour le modèle juge.\n",
    "        dataset: Dataset contenant les prompts (liste de dictionnaires avec clé \"prompt\").\n",
    "        prompt_template: Template pour formater les prompts du juge.\n",
    "        device: Appareil utilisé (\"cuda\" ou \"cpu\").\n",
    "        max_new_tokens: Nombre maximal de tokens générés.\n",
    "        temperature: Température pour la génération.\n",
    "        top_p: Paramètre top-p pour la génération.\n",
    "        subset_size: Taille du sous-ensemble de données à évaluer.\n",
    "        verbose: Si True, affiche les résultats.\n",
    "\n",
    "    Returns:\n",
    "        float: Win rate du modèle de politique (entre 0 et 1).\n",
    "    \"\"\"\n",
    "    @torch.no_grad()\n",
    "    def generate_summary(model, tokenizer, prompts, max_new_tokens, temperature, top_p, device):\n",
    "        \"\"\"Génère des résumés pour une liste de prompts.\"\"\"\n",
    "        enc = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "            padding=True,\n",
    "        ).to(device)\n",
    "\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        responses = []\n",
    "        for i in range(len(prompts)):\n",
    "            prompt_len = int(enc[\"attention_mask\"][i].sum())\n",
    "            gen_ids = out[i][prompt_len:]\n",
    "            text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "            responses.append(text)\n",
    "\n",
    "        return responses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def judge_pairwise(judge_model, judge_tokenizer, formatted_prompt, device):\n",
    "        \"\"\"Utilise le modèle juge pour comparer deux réponses.\"\"\"\n",
    "        inputs = judge_tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "        ).to(device)\n",
    "\n",
    "        outputs = judge_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            eos_token_id=judge_tokenizer.eos_token_id,\n",
    "            pad_token_id=judge_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        gen = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        text = judge_tokenizer.decode(gen, skip_special_tokens=True).strip().upper()\n",
    "\n",
    "        if text.startswith(\"A\"):\n",
    "            return \"A\"\n",
    "        elif text.startswith(\"B\"):\n",
    "            return \"B\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Générer des résumés avec les deux modèles\n",
    "    summaries_a, summaries_b, originals = [], [], []\n",
    "    tokenizer = judge_tokenizer  # On suppose que le tokenizer est le même pour tous les modèles\n",
    "\n",
    "    for ex in tqdm(dataset[:subset_size], desc=\"Génération des résumés\", disable=not verbose):\n",
    "        prompt = ex[\"prompt\"].strip()\n",
    "        if not prompt:\n",
    "            continue\n",
    "\n",
    "        ref_responses = generate_summary(\n",
    "            ref_model, tokenizer, [prompt], max_new_tokens, temperature, top_p, device\n",
    "        )\n",
    "        dpo_responses = generate_summary(\n",
    "            policy_model, tokenizer, [prompt], max_new_tokens, temperature, top_p, device\n",
    "        )\n",
    "\n",
    "        summaries_a.append(dpo_responses[0])\n",
    "        summaries_b.append(ref_responses[0])\n",
    "        originals.append(prompt)\n",
    "\n",
    "    # Évaluer les win rates\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    abstain = 0\n",
    "    total = len(summaries_a)\n",
    "\n",
    "    for sa, sb, txt in tqdm(zip(summaries_a, summaries_b, originals), desc=\"Évaluation des win rates\", disable=not verbose):\n",
    "        prompt = prompt_template.replace(\"{post}\", txt).replace(\"{summary_a}\", sa).replace(\"{summary_b}\", sb)\n",
    "\n",
    "        choice = judge_pairwise(judge_model, judge_tokenizer, prompt, device)\n",
    "\n",
    "        if choice == \"A\":\n",
    "            wins += 1\n",
    "        elif choice == \"B\":\n",
    "            losses += 1\n",
    "        else:\n",
    "            abstain += 1\n",
    "\n",
    "    real_win_rate = wins / total if total > 0 else 0.0\n",
    "    conditional_win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0.0\n",
    "    decision_rate = (wins + losses) / total if total > 0 else 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n=== ÉVALUATION PÉRIODIQUE ===\")\n",
    "        print(f\"Win rate:               {real_win_rate:.3f}\")\n",
    "        print(f\"Conditional win rate:   {conditional_win_rate:.3f}\")\n",
    "        print(f\"Judge decision rate:    {decision_rate:.3f}\")\n",
    "        print(f\"Abstentions:            {abstain}/{total}\")\n",
    "\n",
    "    return real_win_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de112e",
   "metadata": {},
   "source": [
    "# FINAL PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model.eval()\n",
    "\n",
    "\n",
    "for batch in ppo_ds.select(range(batch)):\n",
    "    prompt = build_prompt(batch)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
    "    prompt_len = inputs['input_ids'].size(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summaries = policy_model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_new_tokens=64\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=return_sequence_nb\n",
    "        )\n",
    "\n",
    "        #Preparing for scoring\n",
    "        masks = (summaries != tokenizer.pad_token_id).long()\n",
    "        rm_scores = reward_model(summaries, attention_mask=masks).detach()\n",
    "        best_idx = rm_scores.argmax()\n",
    "        summary = summaries[best_idx].unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "        mask = (summary != tokenizer.pad_token_id).long()\n",
    "\n",
    "        response_mask = get_response_mask(summary, prompt_len)\n",
    "        response_mask_shifted = response_mask[:, 1:]\n",
    "\n",
    "        old_logits = policy_model(summary,attention_mask=mask,).logits\n",
    "        ref_logits = policy_model_ref(summary,attention_mask=mask,).logits\n",
    "\n",
    "        #Computing the log prob\n",
    "        old_logprob = get_log_probs(old_logits, summary).detach()\n",
    "        ref_log_probs = get_log_probs(ref_logits, summary)\n",
    "\n",
    "        #Reward computation\n",
    "        rm_score = reward_model(summary, attention_mask=mask).detach()\n",
    "\n",
    "        #Simulating the human feedback policy\n",
    "        kl_dist = old_logprob - ref_log_probs\n",
    "        rewards = -kl_beta * kl_dist + rm_score\n",
    "        last_idx = response_mask_shifted.sum(dim=1) - 1\n",
    "\n",
    "        #Computing the advantage\n",
    "        values = critic_model(summary, attention_mask=mask)[:,:-1]\n",
    "        advantages, returns = compute_gae(values, rewards, response_mask_shifted)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        advantages = advantages.detach()\n",
    "        returns = returns.detach()\n",
    "\n",
    "    #Actual PPO update\n",
    "    for _ in range(epoch_nb):\n",
    "        logits = policy_model(summary, attention_mask=mask).logits\n",
    "        logprob = get_log_probs(logits, summary)\n",
    "        value = critic_model(summary, attention_mask=mask)[:,:-1]\n",
    "\n",
    "        #Clipping ratio\n",
    "        ratio = torch.exp(logprob - old_logprob)\n",
    "\n",
    "        #Policy loss\n",
    "        clip1 = ratio * advantages\n",
    "        clip2 = torch.clamp(ratio, 1 - clip_range, 1 + clip_range) * advantages\n",
    "        policy_loss = -(torch.min(clip1, clip2) * response_mask_shifted).sum() / response_mask_shifted.sum()\n",
    "        \n",
    "        value_loss = (F.mse_loss(value, returns, reduction='none') * response_mask_shifted).sum() / response_mask_shifted.sum()\n",
    "\n",
    "        loss = policy_loss + beta * value_loss\n",
    "\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        critic_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item():.4f} | RM Score: {rm_score.mean().item():.4f} | KL: {kl_dist.mean().item():.4f}\")\n",
    "\n",
    "        if epoch % 5 == 0:  # Évaluer toutes les 5 époques\n",
    "            win_rate = evaluate_periodic_win_rate(\n",
    "                policy_model=policy_model,\n",
    "                ref_model=policy_model_ref,\n",
    "                judge_model=judge_model,  # Assurez-vous que judge_model est défini\n",
    "                judge_tokenizer=judge_tokenizer,  # Assurez-vous que judge_tokenizer est défini\n",
    "                dataset=ppo_ds,  # Utilisez le même dataset ou un dataset de validation\n",
    "                prompt_template=prompt_template,\n",
    "                device=device,\n",
    "                subset_size=100,  # Évaluer sur un sous-ensemble de 100 exemples\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "output_dir = \"/saved_models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Sauvegarder le modèle de politique\n",
    "policy_model_path = os.path.join(output_dir, \"policy_model.pt\")\n",
    "torch.save({\n",
    "    'epoch': epoch_nb,\n",
    "    'model_state_dict': policy_model.state_dict(),\n",
    "    'optimizer_state_dict': policy_optimizer.state_dict(),\n",
    "    'loss': loss.item(),\n",
    "}, policy_model_path)\n",
    "\n",
    "# Sauvegarder le modèle de critique\n",
    "critic_model_path = os.path.join(output_dir, \"critic_model.pt\")\n",
    "torch.save({\n",
    "    'epoch': epoch_nb,\n",
    "    'model_state_dict': critic_model.state_dict(),\n",
    "    'optimizer_state_dict': critic_optimizer.state_dict(),\n",
    "}, critic_model_path)\n",
    "\n",
    "print(f\"Modèles sauvegardés dans {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
