{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c107861e",
   "metadata": {},
   "source": [
    "This code is based on the paper provided by Stiennon et al. that shows how to use PPO to train a LM using human feedback for a summarization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f18f63",
   "metadata": {},
   "source": [
    "# IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eda0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246f089",
   "metadata": {},
   "source": [
    "# LOADING SFT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "policy_model_ref = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2451aa",
   "metadata": {},
   "source": [
    "# LOADING REDDIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"openai/summarize_from_feedback\", \"comparisons\")\n",
    "\n",
    "def build_prompt(example):\n",
    "    return f\"Summarize the following text:\\n{example['text']}\\nSummary:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dpo(dataset):\n",
    "    return dataset.map(\n",
    "        lambda x: {\n",
    "            \"prompt\": x[\"info\"][\"post\"].strip(),\n",
    "            \"chosen\": x[\"summaries\"][x[\"choice\"]][\"text\"].strip(),\n",
    "            \"rejected\": x[\"summaries\"][1 - x[\"choice\"]][\"text\"].strip(),\n",
    "        },\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "ppo_ds = format_dpo(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687b523",
   "metadata": {},
   "source": [
    "# TRAINING REWARD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4477d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_lm):\n",
    "        super().__init__()\n",
    "        self.lm = base_lm\n",
    "        self.reward_head = nn.Linear(base_lm.config.hidden_size, 1)\n",
    "\n",
    "\n",
    "def forward(self, input_ids, attention_mask):\n",
    "    outputs = self.lm(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "    output_hidden_states=True\n",
    "    )\n",
    "    last_hidden = outputs.hidden_states[-1][:, -1]\n",
    "    return self.reward_head(last_hidden).squeeze(-1)\n",
    "\n",
    "\n",
    "reward_model = RewardModel(\n",
    "    AutoModelForCausalLM.from_pretrained(model_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(reward_model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "for batch in ppo_ds.select(range(50)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    inputs_w = tokenizer(batch['prompt'] + batch['chosen'], return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs_l = tokenizer(batch['prompt'] + batch['rejected'], return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "\n",
    "    r_w = reward_model(**inputs_w)\n",
    "    r_l = reward_model(**inputs_l)\n",
    "\n",
    "\n",
    "    loss = -torch.log(torch.sigmoid(r_w - r_l)).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d5366",
   "metadata": {},
   "source": [
    "# TRAINING PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74278951",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=2,\n",
    "    target_kl=0.1\n",
    ")\n",
    "\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=policy_model_ref,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model.eval()\n",
    "\n",
    "\n",
    "for batch in dataset.select(range(10)):\n",
    "    prompt = build_prompt(batch)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "\n",
    "    response = ppo_trainer.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_new_tokens=64\n",
    "    )\n",
    "\n",
    "\n",
    "    summary = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "    reward_inputs = tokenizer(prompt + summary, return_tensors='pt', truncation=True)\n",
    "    reward = reward_model(**reward_inputs)\n",
    "\n",
    "\n",
    "    ppo_trainer.step(\n",
    "    inputs['input_ids'],\n",
    "    response,\n",
    "    reward\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
