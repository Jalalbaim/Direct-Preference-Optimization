{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593f73f1",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9564307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation pour Colab (Python 3.12+)\n",
    "!pip install --upgrade pip setuptools wheel -q\n",
    "!pip install transformers[torch] datasets trl wandb accelerate -q\n",
    "\n",
    "# V√©rification\n",
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "print(\"‚úÖ Installation r√©ussie!\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"TRL: {trl.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f406b",
   "metadata": {},
   "source": [
    "## 0. Mount Google Drive (Optional - for Colab/Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive pour √©conomiser temps et quota\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    SAVE_BASE_PATH = '/content/drive/MyDrive/dpo_ppo_training'\n",
    "    os.makedirs(SAVE_BASE_PATH, exist_ok=True)\n",
    "    print(f\"‚úÖ Google Drive mont√©. Mod√®les sauvegard√©s sur: {SAVE_BASE_PATH}\")\n",
    "    USE_DRIVE = True\n",
    "except ImportError:\n",
    "    # Pas sur Colab\n",
    "    SAVE_BASE_PATH = './results'\n",
    "    USE_DRIVE = False\n",
    "    print(f\"‚ö†Ô∏è  Pas de Google Drive d√©tect√©. Stockage local: {SAVE_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d17736",
   "metadata": {},
   "source": [
    "## 1. Configuration DPO\n",
    "\n",
    "**Param√®tres configurables :**\n",
    "- `beta` : Param√®tre de r√©gularisation KL (0.1, 0.5, 1.0 typiques)\n",
    "- `batch_size` : √Ä ajuster selon GPU T4\n",
    "- `learning_rate` : Taux d'apprentissage\n",
    "- `num_epochs` : Nombre d'√©poques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "# =====================================\n",
    "# CONFIGURATION PRINCIPALE\n",
    "# =====================================\n",
    "\n",
    "# DPO Hyperparameters\n",
    "BETA = 0.1  # Param√®tre de r√©gularisation KL (0.1, 0.5, 1.0 selon papier DPO)\n",
    "BATCH_SIZE = 4  # √Ä ajuster selon votre GPU T4 (4-8 pour DPO)\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Batch effectif = 4 * 4 = 16\n",
    "LEARNING_RATE = 1e-6  # LR pour DPO avec BF16 (1e-6 stable, 5e-7 tr√®s conservateur)\n",
    "NUM_EPOCHS = 1\n",
    "MAX_LENGTH = 512  # Longueur maximale des s√©quences\n",
    "MAX_PROMPT_LENGTH = 128  # Longueur maximale du prompt\n",
    "\n",
    "# Model paths\n",
    "SFT_MODEL_PATH = f\"{SAVE_BASE_PATH}/sft_model\"\n",
    "DPO_MODEL_PATH = f\"{SAVE_BASE_PATH}/dpo_model_beta{BETA}\"\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"DPO Configuration\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Beta (KL regularization): {BETA}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Max Length: {MAX_LENGTH}\")\n",
    "print(f\"Max Prompt Length: {MAX_PROMPT_LENGTH}\")\n",
    "print(f\"SFT Model: {SFT_MODEL_PATH}\")\n",
    "print(f\"DPO Model Output: {DPO_MODEL_PATH}\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e24954",
   "metadata": {},
   "source": [
    "## 2. Load Preference Pairs Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 1: Chargement du dataset de paires de pr√©f√©rences\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load preference pairs\n",
    "pairs_path = f\"{SAVE_BASE_PATH}/datasets/preference_pairs.json\"\n",
    "\n",
    "if not os.path.exists(pairs_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Dataset introuvable: {pairs_path}\\n\"\n",
    "        \"Veuillez d'abord ex√©cuter le notebook de g√©n√©ration des paires de pr√©f√©rences.\"\n",
    "    )\n",
    "\n",
    "# Load pairs\n",
    "print(f\"\\nüì• Chargement des paires depuis: {pairs_path}\")\n",
    "with open(pairs_path, 'r', encoding='utf-8') as f:\n",
    "    preference_pairs = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ {len(preference_pairs)} paires de pr√©f√©rences charg√©es\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nExemples de paires:\")\n",
    "for i in range(min(2, len(preference_pairs))):\n",
    "    pair = preference_pairs[i]\n",
    "    print(f\"\\nPaire {i+1}:\")\n",
    "    print(f\"  Prompt:   {pair['prompt'][:60]}...\")\n",
    "    print(f\"  Chosen:   {pair['chosen'][:60]}...\")\n",
    "    print(f\"  Rejected: {pair['rejected'][:60]}...\")\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "dpo_dataset = Dataset.from_dict({\n",
    "    \"prompt\": [pair[\"prompt\"] for pair in preference_pairs],\n",
    "    \"chosen\": [pair[\"chosen\"] for pair in preference_pairs],\n",
    "    \"rejected\": [pair[\"rejected\"] for pair in preference_pairs],\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset DPO cr√©√© avec {len(dpo_dataset)} paires\")\n",
    "print(f\"   Colonnes: {dpo_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1dafa9",
   "metadata": {},
   "source": [
    "## 3. Load Models (Policy & Reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76588246",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 2: Chargement des mod√®les\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if SFT model exists\n",
    "if not os.path.exists(SFT_MODEL_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Mod√®le SFT introuvable: {SFT_MODEL_PATH}\\n\"\n",
    "        \"Veuillez d'abord entra√Æner le mod√®le SFT.\"\n",
    "    )\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nüì• Chargement du tokenizer depuis: {SFT_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(\"‚úÖ Tokenizer charg√©\")\n",
    "\n",
    "# Load policy model (will be trained)\n",
    "print(f\"\\nüì• Chargement du policy model (SFT)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16  # BF16 optimal pour DPO (stable + rapide)\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    print(f\"‚úÖ Policy model charg√© sur GPU ({model.num_parameters() / 1e9:.2f}B params)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Policy model charg√© sur CPU\")\n",
    "\n",
    "# Load reference model (frozen SFT)\n",
    "print(f\"\\nüì• Chargement du reference model (SFT frozen)...\")\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16  # BF16 optimal pour DPO (stable + rapide)\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    ref_model = ref_model.to(\"cuda\")\n",
    "    print(f\"‚úÖ Reference model charg√© sur GPU (frozen)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Reference model charg√© sur CPU\")\n",
    "\n",
    "# Freeze reference model\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "ref_model.eval()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Mod√®les charg√©s avec succ√®s\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38df0e4",
   "metadata": {},
   "source": [
    "## 4. Initialize W&B Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be63d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B\n",
    "wandb.login()\n",
    "\n",
    "# Initialize W&B run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"dpo_imdb_beta{BETA}_{timestamp}\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"dpo_ppo\",\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"model\": \"gpt2-large\",\n",
    "        \"dataset\": \"imdb_preference_pairs\",\n",
    "        \"num_pairs\": len(dpo_dataset),\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"effective_batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"beta\": BETA,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"max_prompt_length\": MAX_PROMPT_LENGTH,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ W&B initialized: {run_name}\")\n",
    "print(f\"   Project: dpo_ppo\")\n",
    "print(f\"   Beta: {BETA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90f8c1",
   "metadata": {},
   "source": [
    "## 5. Configure DPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32410a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOConfig\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 3: Configuration du DPO Trainer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# DPO Configuration (remplace TrainingArguments + beta)\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=DPO_MODEL_PATH,\n",
    "    \n",
    "    # DPO-specific\n",
    "    beta=BETA,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    warmup_steps=50,  # Warmup plus long pour stabilit√©\n",
    "    weight_decay=0.05,\n",
    "    max_grad_norm=1.0,  # CRITIQUE: gradient clipping pour √©viter explosion\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Optimization - BF16 activ√© (meilleur que FP16 pour DPO)\n",
    "    fp16=False,  # Ne jamais utiliser FP16 avec DPO (instable)\n",
    "    bf16=True,  # BF16 stable pour DPO (m√™me plage que FP32)\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Other\n",
    "    remove_unused_columns=False,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Initialize DPO Trainer\n",
    "print(f\"\\nüöÄ Initialisation du DPO Trainer...\")\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=dpo_dataset,\n",
    "    processing_class=tokenizer,  # tokenizer pass√© via processing_class dans TRL r√©cent\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DPO Trainer initialis√©\")\n",
    "print(f\"\\nüìä Configuration:\")\n",
    "print(f\"   - Beta: {BETA}\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"   - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"   - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   - Total pairs: {len(dpo_dataset)}\")\n",
    "print(f\"   - Estimated steps: {len(dpo_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}\")\n",
    "print(f\"   - FP16: {dpo_config.fp16}\")\n",
    "print(f\"   - BF16: {dpo_config.bf16}\")\n",
    "print(f\"   - Gradient checkpointing: {dpo_config.gradient_checkpointing}\")\n",
    "print(f\"   - Max grad norm: {dpo_config.max_grad_norm}\")\n",
    "\n",
    "print(f\"\\n‚úÖ BF16 activ√© : stable pour DPO (m√™me plage dynamique que FP32, 2x plus rapide)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b3942",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 4: Entra√Ænement DPO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüöÄ D√©marrage de l'entra√Ænement DPO...\")\n",
    "print(f\"   - {len(dpo_dataset)} paires de pr√©f√©rences\")\n",
    "print(f\"   - {NUM_EPOCHS} epoch(s)\")\n",
    "print(f\"   - Beta = {BETA}\\n\")\n",
    "\n",
    "# Clear GPU cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ GPU cache cleared\\n\")\n",
    "\n",
    "# Resume from last checkpoint if available\n",
    "from pathlib import Path\n",
    "RESUME_FROM_CHECKPOINT = True\n",
    "resume_checkpoint = None\n",
    "if RESUME_FROM_CHECKPOINT:\n",
    "    ckpts = sorted(\n",
    "        Path(DPO_MODEL_PATH).glob(\"checkpoint-*/\"),\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1,\n",
    "    )\n",
    "    if ckpts:\n",
    "        resume_checkpoint = str(ckpts[-1])\n",
    "        print(f\"üîÑ Reprise depuis le checkpoint: {resume_checkpoint}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Aucun checkpoint trouv√©, entra√Ænement from scratch\")\n",
    "\n",
    "# Train\n",
    "dpo_trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Entra√Ænement DPO termin√©!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ff043",
   "metadata": {},
   "source": [
    "## 6. DPO Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db6c57f",
   "metadata": {},
   "source": [
    "## 7. Save DPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 5: Sauvegarde du mod√®le DPO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save model\n",
    "print(f\"\\nüíæ Sauvegarde du mod√®le DPO dans: {DPO_MODEL_PATH}\")\n",
    "dpo_trainer.save_model(DPO_MODEL_PATH)\n",
    "tokenizer.save_pretrained(DPO_MODEL_PATH)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le DPO sauvegard√©!\")\n",
    "print(f\"\\nüìÅ Fichiers cr√©√©s:\")\n",
    "print(f\"   - {DPO_MODEL_PATH}/pytorch_model.bin\")\n",
    "print(f\"   - {DPO_MODEL_PATH}/config.json\")\n",
    "print(f\"   - {DPO_MODEL_PATH}/tokenizer.json\")\n",
    "\n",
    "# Close W&B run\n",
    "wandb.finish()\n",
    "print(f\"\\n‚úÖ W&B run closed\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ DPO TRAINING COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nüéØ Prochaines √©tapes:\")\n",
    "print(f\"   1. √âvaluer le mod√®le DPO sur le test set\")\n",
    "print(f\"   2. Comparer avec SFT et PPO\")\n",
    "print(f\"   3. Tester d'autres valeurs de beta (0.5, 1.0)\")\n",
    "print(f\"   4. G√©n√©rer la courbe reward-KL (Figure 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b4030",
   "metadata": {},
   "source": [
    "## 8. Test DPO Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced24896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Test du mod√®le DPO sur quelques exemples\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load DPO model for testing\n",
    "test_model = AutoModelForCausalLM.from_pretrained(DPO_MODEL_PATH)\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(DPO_MODEL_PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    test_model = test_model.to(\"cuda\")\n",
    "\n",
    "test_model.eval()\n",
    "\n",
    "# Test on 5 random prompts from dataset\n",
    "import random\n",
    "random.seed(42)\n",
    "test_indices = random.sample(range(len(dpo_dataset)), 5)\n",
    "test_samples = dpo_dataset.select(test_indices)\n",
    "\n",
    "print(f\"\\nG√©n√©ration sur 5 prompts al√©atoires du dataset:\\n\")\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    prompt = sample[\"prompt\"]\n",
    "    chosen = sample[\"chosen\"]\n",
    "    rejected = sample[\"rejected\"]\n",
    "    \n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = test_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=test_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    continuation = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"\\nDPO Generated: {continuation[:100]}...\")\n",
    "    print(f\"\\nOriginal Chosen:   {chosen[len(prompt):100]}...\")\n",
    "    print(f\"Original Rejected: {rejected[len(prompt):100]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚úÖ Test termin√©!\")\n",
    "print(f\"\\nüí° Le mod√®le DPO devrait g√©n√©rer du texte plus proche de 'chosen' que de 'rejected'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf582c79",
   "metadata": {},
   "source": [
    "## 9. Compare SFT vs DPO (Side-by-side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fee9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Comparaison SFT vs DPO sur les m√™mes prompts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load SFT model\n",
    "print(\"\\nüì• Chargement du mod√®le SFT...\")\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(SFT_MODEL_PATH)\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sft_model = sft_model.to(\"cuda\")\n",
    "\n",
    "sft_model.eval()\n",
    "print(\"‚úÖ Mod√®le SFT charg√©\")\n",
    "\n",
    "# Test on 3 prompts\n",
    "test_prompts = [\n",
    "    \"This movie is\",\n",
    "    \"I really enjoyed\",\n",
    "    \"The acting was\"\n",
    "]\n",
    "\n",
    "print(f\"\\nG√©n√©ration comparative sur {len(test_prompts)} prompts:\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # SFT generation\n",
    "    inputs_sft = sft_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs_sft = inputs_sft.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_sft = sft_model.generate(\n",
    "            **inputs_sft,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=sft_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    sft_text = sft_tokenizer.decode(outputs_sft[0], skip_special_tokens=True)\n",
    "    sft_continuation = sft_text[len(prompt):].strip()\n",
    "    \n",
    "    # DPO generation\n",
    "    inputs_dpo = test_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs_dpo = inputs_dpo.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_dpo = test_model.generate(\n",
    "            **inputs_dpo,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=test_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    dpo_text = test_tokenizer.decode(outputs_dpo[0], skip_special_tokens=True)\n",
    "    dpo_continuation = dpo_text[len(prompt):].strip()\n",
    "    \n",
    "    # Display comparison\n",
    "    print(f\"\\nüìù SFT:  {sft_continuation}\")\n",
    "    print(f\"üéØ DPO:  {dpo_continuation}\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚úÖ Comparaison termin√©e!\")\n",
    "print(f\"\\nüí° DPO devrait g√©n√©rer du texte avec un sentiment plus positif (selon les pr√©f√©rences)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
