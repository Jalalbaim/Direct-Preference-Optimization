# RLHF Methods Comparison: DPO vs PPO vs GRPO

ImplÃ©mentation et comparaison de trois mÃ©thodes d'alignement pour les modÃ¨les de langage :

- **DPO** (Direct Preference Optimization) - Offline learning from preferences
- **PPO** (Proximal Policy Optimization) - Online RL with reward model
- **GRPO** (Group Relative Policy Optimization) - Simplified PPO with group normalization

## Models

- **y_w, y_l generation**: Ollama gemma3:4b (preference pairs creation)
- **SFT Model**: TinyLlama-1.1B-Chat-v1.0
- **Methods**: DPO, PPO, GRPO
- **Reward Model**: DistilBERT sentiment classifier

## Features

This project implements three state-of-the-art alignment methods:

### DPO (Direct Preference Optimization)
- âœ… Preference data preparation (chosen/rejected pairs)
- âœ… Training with DPO loss (Î²=0.1)
- âœ… No reward model needed
- âœ… Simple and stable

### PPO (Proximal Policy Optimization)
- âœ… Online generation with reward feedback
- âœ… Value function for advantage estimation
- âœ… Clipped surrogate objective
- âœ… Entropy bonus for exploration

### GRPO (Group Relative Policy Optimization)
- âœ… Group-based response generation
- âœ… Reward normalization within groups
- âœ… No value function needed
- âœ… Reduced variance

## Quick Start

```bash
# Install requirements
pip install -r requirements.txt

# Prepare data
python scripts/prepare_sentiment_data.py  # For DPO
python scripts/prepare_prompts.py         # For PPO/GRPO

# Train with each method
python scripts/train_sentiment.py  # DPO
python scripts/train_ppo.py        # PPO
python scripts/train_grpo.py       # GRPO

# Compare results
python scripts/compare_methods.py
```

## Documentation

- ðŸ“˜ [**QUICKSTART.md**](QUICKSTART.md) - Guide de dÃ©marrage rapide
- ðŸ“— [**COMPARISON_GUIDE.md**](COMPARISON_GUIDE.md) - Guide dÃ©taillÃ© de comparaison

## Project Structure

```
â”œâ”€â”€ src/dpo/
â”‚   â”œâ”€â”€ losses.py           # DPO loss
â”‚   â”œâ”€â”€ ppo_losses.py       # PPO losses
â”‚   â”œâ”€â”€ grpo_losses.py      # GRPO losses
â”‚   â”œâ”€â”€ reward_models.py    # Reward models
â”‚   â”œâ”€â”€ trainer.py          # DPO trainer
â”‚   â”œâ”€â”€ ppo_trainer.py      # PPO trainer
â”‚   â””â”€â”€ grpo_trainer.py     # GRPO trainer
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ sentiment.yaml      # DPO config
â”‚   â”œâ”€â”€ ppo_sentiment.yaml  # PPO config
â”‚   â””â”€â”€ grpo_sentiment.yaml # GRPO config
â””â”€â”€ scripts/
    â”œâ”€â”€ train_sentiment.py  # Train DPO
    â”œâ”€â”€ train_ppo.py        # Train PPO
    â”œâ”€â”€ train_grpo.py       # Train GRPO
    â””â”€â”€ compare_methods.py  # Compare all methods
```

## Evaluation

Run the comparison script to evaluate all three methods:

```bash
python scripts/compare_methods.py
```

This will generate a detailed comparison with:
- Mean reward scores
- Standard deviation
- Sample generations
- Results saved to `comparison_results.json`
