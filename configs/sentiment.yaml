data:
  max_prompt_length: 256
  max_response_length: 128
  train_path: data/processed/sentiment/train_preferences.jsonl
  val_path: data/processed/sentiment/val_preferences.jsonl
dpo:
  beta: 0.01
  label_smoothing: 0.0
experiment_name: dpo_sentiment_tinyllama
logging:
  eval_every: 500
  log_every: 50
  save_dir: checkpoints/sentiment_dpo_beta_0.01
model:
  dtype: bfloat16
  name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  use_flash_attention: false
training:
  batch_size: 1
  grad_accumulation_steps: 16
  learning_rate: 1e-5
  max_grad_norm: 1.0
  num_epochs: 1
  seed: 42
  warmup_steps: 100
  weight_decay: 0.01
