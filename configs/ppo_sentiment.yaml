experiment_name: "ppo_sentiment_tinyllama"

model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  dtype: "bfloat16"
  use_flash_attention: false

training:
  seed: 42
  num_epochs: 1
  batch_size: 4
  learning_rate: 1e-5
  weight_decay: 0.01
  max_grad_norm: 1.0
  warmup_steps: 100
  gradient_checkpointing: true    # Économie ~30-40% VRAM activations
  use_8bit_optimizer: true        # Économie ~50% VRAM optimiseur

ppo:
  clip_epsilon: 0.2        # PPO clipping epsilon
  value_coef: 0.5          # Coefficient pour value loss
  entropy_coef: 0.01       # Coefficient pour entropy bonus
  gamma: 0.99              # Discount factor
  gae_lambda: 0.95         # GAE lambda
  num_ppo_epochs: 4        # Nombre d'epochs PPO par batch
  target_kl: 0.01          # Target KL for early stopping (0.01 = 1% divergence)
  use_kl_early_stop: true  # Activer l'arrêt précoce basé sur KL

reward_model:
  name: "lvwerra/distilbert-imdb"

generation:
  max_length: 128
  temperature: 0.8
  top_p: 0.9

data:
  prompt_path: "data/processed/sentiment/prompts.jsonl"
  max_prompt_length: 256

logging:
  save_dir: "checkpoints/sentiment_ppo"
  log_every: 3
  eval_every: 25
  wandb_enabled: true
  wandb_project: "ppo-sentiment"
  wandb_run_name: "ppo_tinyllama_no2"
