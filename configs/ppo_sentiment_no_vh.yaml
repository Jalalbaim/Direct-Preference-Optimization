experiment_name: "ppo_sentiment_no_value_head"

model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  dtype: "bfloat16"
  use_flash_attention: false

training:
  seed: 42
  num_epochs: 1
  batch_size: 2
  learning_rate: 1e-5
  weight_decay: 0.01
  max_grad_norm: 1.0
  warmup_steps: 100
  gradient_checkpointing: true    # Économie ~30-40% VRAM activations
  use_8bit_optimizer: true        # Économie ~50% VRAM optimiseur

ppo:
  clip_epsilon: 0.2        # PPO clipping epsilon
  entropy_coef: 0.01       # Coefficient pour entropy bonus (PAS de value_coef)
  gamma: 0.99              # Discount factor (moins utilisé sans GAE)
  num_ppo_epochs: 4        # Nombre d'epochs PPO par batch
  target_kl: 0.01          # Target KL for early stopping
  use_kl_early_stop: true  # Activer l'arrêt précoce basé sur KL

reward_model:
  name: "lvwerra/distilbert-imdb"

generation:
  max_length: 128
  temperature: 0.8
  top_p: 0.9

data:
  prompt_path: "data/processed/sentiment/prompts.jsonl"
  max_prompt_length: 256

logging:
  save_dir: "checkpoints/sentiment_ppo_no_vh"
  log_every: 10
  eval_every: 100
