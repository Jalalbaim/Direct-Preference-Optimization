{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b144e3f9",
   "metadata": {},
   "source": [
    "## 1. Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de70e75d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2681187856.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_collate_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo_trainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Ajouter le r√©pertoire racine au path\n",
    "ROOT = os.path.abspath('..')\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.append(ROOT)\n",
    "\n",
    "from src.dpo.models import load_models\n",
    "from src.dpo.data import PromptDataset, prompt_collate_fn\n",
    "from src.ppo.ppo_trainer import PPOTrainer\n",
    "from src.dpo.utils import load_yaml_config\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ed0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "   \n",
    "\n",
    "print(f\"Device s√©lectionn√©: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d664eb",
   "metadata": {},
   "source": [
    "## 3. V√©rification et Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier si les donn√©es de prompts existent\n",
    "prompts_path = \"data/processed/sentiment/prompts.jsonl\"\n",
    "\n",
    "if not os.path.exists(prompts_path):\n",
    "    print(\"‚ö†Ô∏è  Fichier prompts.jsonl non trouv√©!\")\n",
    "    print(\"üì• Ex√©cution de prepare_prompts.py...\")\n",
    "    !python scripts/prepare_prompts.py\n",
    "    print(\"‚úÖ Prompts pr√©par√©s\")\n",
    "else:\n",
    "    # Compter le nombre de prompts\n",
    "    with open(prompts_path, 'r') as f:\n",
    "        num_prompts = sum(1 for line in f if line.strip())\n",
    "    print(f\"‚úÖ {num_prompts} prompts trouv√©s dans {prompts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73358b",
   "metadata": {},
   "source": [
    "## 4. Configuration des Hyperparam√®tres\n",
    "\n",
    "Vous pouvez modifier ces param√®tres selon vos besoins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger la config par d√©faut\n",
    "config_path = \"configs/ppo_sentiment.yaml\"\n",
    "config = load_yaml_config(config_path)\n",
    "\n",
    "# Afficher les param√®tres principaux\n",
    "print(\"üìä Configuration PPO:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(\"\\nüéØ Param√®tres PPO:\")\n",
    "print(f\"  Clip epsilon: {config['ppo']['clip_epsilon']}\")\n",
    "print(f\"  Value coef: {config['ppo']['value_coef']}\")\n",
    "print(f\"  Entropy coef: {config['ppo']['entropy_coef']}\")\n",
    "print(f\"  Target KL: {config['ppo']['target_kl']}\")\n",
    "print(f\"  PPO epochs: {config['ppo']['num_ppo_epochs']}\")\n",
    "print(\"\\nüéÆ G√©n√©ration:\")\n",
    "print(f\"  Max length: {config['generation']['max_length']}\")\n",
    "print(f\"  Temperature: {config['generation']['temperature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a3adf",
   "metadata": {},
   "source": [
    "### Modifier les param√®tres (optionnel)\n",
    "\n",
    "D√©commentez et modifiez si vous voulez changer certains param√®tres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec978b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de modifications\n",
    "# config['training']['batch_size'] = 1  # R√©duire si probl√®me de m√©moire\n",
    "# config['training']['num_epochs'] = 2  # Plus d'epochs\n",
    "# config['ppo']['num_ppo_epochs'] = 2   # Moins d'epochs PPO par batch\n",
    "# config['generation']['max_length'] = 64  # R√©ponses plus courtes\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration personnalis√©e (si modifi√©e)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d12637",
   "metadata": {},
   "source": [
    "## 5. Chargement des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Chargement des mod√®les...\")\n",
    "print(\"   Cela peut prendre quelques minutes...\")\n",
    "\n",
    "model_name = config[\"model\"][\"name\"]\n",
    "dtype = config[\"model\"][\"dtype\"]\n",
    "\n",
    "# Charger les mod√®les (policy et r√©f√©rence)\n",
    "mb = load_models(model_name, dtype=dtype, device=device)\n",
    "tokenizer = mb.tokenizer\n",
    "\n",
    "print(f\"‚úÖ Mod√®les charg√©s: {model_name}\")\n",
    "print(f\"   Policy model: {mb.policy_model.num_parameters():,} param√®tres\")\n",
    "print(f\"   Device: {mb.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ead1b",
   "metadata": {},
   "source": [
    "## 6. Pr√©paration du DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb1cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset de prompts\n",
    "prompt_dataset = PromptDataset(config[\"data\"][\"prompt_path\"])\n",
    "max_prompt_length = config[\"data\"][\"max_prompt_length\"]\n",
    "\n",
    "print(f\"üìö Dataset charg√©: {len(prompt_dataset)} prompts\")\n",
    "\n",
    "# Fonction de collate\n",
    "def collate(batch):\n",
    "    return prompt_collate_fn(\n",
    "        batch,\n",
    "        tokenizer=tokenizer,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "    )\n",
    "\n",
    "# DataLoader\n",
    "prompt_loader = DataLoader(\n",
    "    prompt_dataset,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoader cr√©√©: {len(prompt_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14557e",
   "metadata": {},
   "source": [
    "## 7. Initialisation du PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèóÔ∏è  Initialisation du PPO Trainer...\")\n",
    "\n",
    "# Cr√©er le trainer\n",
    "trainer = PPOTrainer(\n",
    "    model_bundle=mb,\n",
    "    prompt_loader=prompt_loader,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialis√©\")\n",
    "print(f\"   Reward model: {config['reward_model']['name']}\")\n",
    "print(f\"   Save dir: {config['logging']['save_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30aecff",
   "metadata": {},
   "source": [
    "## 8. Entra√Ænement PPO üöÄ\n",
    "\n",
    "**‚ö†Ô∏è Attention**: L'entra√Ænement peut prendre du temps, surtout sur CPU!\n",
    "\n",
    "**Temps estim√©**:\n",
    "- Sur GPU: ~30 min - 1h\n",
    "- Sur CPU: 2-4h (voire plus selon le hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fe4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"üöÄ D√©marrage de l'entra√Ænement PPO\")\n",
    "print(f\"   Exp√©rience: {config['experiment_name']}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Lancer l'entra√Ænement\n",
    "    trainer.train()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚úÖ Entra√Ænement termin√©!\")\n",
    "    print(f\"   Temps total: {elapsed/60:.2f} minutes\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è  Entra√Ænement interrompu par l'utilisateur\")\n",
    "    print(\"   Les checkpoints partiels ont √©t√© sauvegard√©s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erreur pendant l'entra√Ænement: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bcd2c",
   "metadata": {},
   "source": [
    "## 9. V√©rification des Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "save_dir = config['logging']['save_dir']\n",
    "checkpoints = glob.glob(os.path.join(save_dir, \"*.pt\"))\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"‚úÖ {len(checkpoints)} checkpoint(s) sauvegard√©(s):\")\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        size_mb = os.path.getsize(ckpt) / (1024**2)\n",
    "        print(f\"   üìÅ {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Aucun checkpoint trouv√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672af94",
   "metadata": {},
   "source": [
    "## 10. Test de G√©n√©ration Rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b26dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la g√©n√©ration avec le mod√®le entra√Æn√©\n",
    "test_prompts = [\n",
    "    \"The movie was\",\n",
    "    \"I think this product is\",\n",
    "    \"The customer service was\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Test de g√©n√©ration avec le mod√®le PPO:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mb.policy_model.eval()\n",
    "with torch.no_grad():\n",
    "    for prompt in test_prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = mb.policy_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\nüí¨ Prompt: {prompt}\")\n",
    "        print(f\"‚ú® Response: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcee02",
   "metadata": {},
   "source": [
    "## üéØ Prochaines √âtapes\n",
    "\n",
    "Maintenant que l'entra√Ænement est termin√©, vous pouvez:\n",
    "\n",
    "1. **√âvaluer le mod√®le** avec `eval_sentiment2.py`:\n",
    "   ```bash\n",
    "   python scripts/eval_sentiment2.py --method ppo --num_samples 3\n",
    "   ```\n",
    "\n",
    "2. **Comparer avec DPO et GRPO**:\n",
    "   ```bash\n",
    "   python scripts/compare_methods.py\n",
    "   ```\n",
    "\n",
    "3. **Visualiser les r√©sultats**:\n",
    "   ```bash\n",
    "   python scripts/visualize_comparison.py\n",
    "   ```\n",
    "\n",
    "4. **Modifier les hyperparam√®tres** et r√©entra√Æner pour comparer les performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc71b6e",
   "metadata": {},
   "source": [
    "## üìä Notes\n",
    "\n",
    "- Les checkpoints sont sauvegard√©s dans `checkpoints/sentiment_ppo/`\n",
    "- Le mod√®le PPO inclut un **value head** en plus de la policy\n",
    "- L'entra√Ænement utilise un **reward model** bas√© sur le sentiment\n",
    "- Le **target KL** permet l'early stopping pour la stabilit√©\n",
    "- Sur CPU, l'entra√Ænement est plus lent mais plus stable que sur MPS (macOS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
