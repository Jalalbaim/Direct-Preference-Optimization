{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a14e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Direct-Preference-Optimization'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
      "remote: Total 81 (delta 22), reused 48 (delta 17), pack-reused 18 (from 1)\u001b[K\n",
      "Receiving objects: 100% (81/81), 1.25 MiB | 20.25 MiB/s, done.\n",
      "Resolving deltas: 100% (22/22), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone -b jeremy --single-branch https://github.com/Jalalbaim/Direct-Preference-Optimization.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f001b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8451a6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Direct-Preference-Optimization\n",
      " R√©pertoire actuel: /content/Direct-Preference-Optimization\n",
      " Contenu: ['.git', 'scripts', '.gitignore', 'src', 'README.md', 'data', 'train_ppo.ipynb', 'requirements.txt', 'configs']\n"
     ]
    }
   ],
   "source": [
    "# Changer de r√©pertoire vers le projet clon√©\n",
    "%cd Direct-Preference-Optimization/\n",
    "\n",
    "import os\n",
    "print(f\" R√©pertoire actuel: {os.getcwd()}\")\n",
    "print(f\" Contenu: {os.listdir('.')[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc78d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installation silencieuse des packages n√©cessaires\n",
    "!pip install torch transformers datasets accelerate sentencepiece protobuf tqdm pyyaml scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a7988",
   "metadata": {},
   "source": [
    "### √âtape 2: Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6d5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144e3f9",
   "metadata": {},
   "source": [
    "## 1. Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de70e75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Working directory: /content/Direct-Preference-Optimization\n",
      "Imports r√©ussis\n",
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Ajouter le r√©pertoire racine au path (d√©j√† dans le bon r√©pertoire apr√®s %cd)\n",
    "ROOT = os.path.abspath('.')\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.append(ROOT)\n",
    "\n",
    "print(f\" Working directory: {os.getcwd()}\")\n",
    "\n",
    "from src.dpo.models import load_models\n",
    "from src.dpo.data import PromptDataset, prompt_collate_fn\n",
    "from src.ppo.ppo_trainer import PPOTrainer\n",
    "from src.dpo.utils import load_yaml_config\n",
    "\n",
    "print(\"Imports r√©ussis\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9ed0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device s√©lectionn√©: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "   \n",
    "\n",
    "print(f\"Device s√©lectionn√©: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d664eb",
   "metadata": {},
   "source": [
    "## 3. V√©rification et Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc6cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497 prompts trouv√©s dans data/processed/sentiment/prompts.jsonl\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier si les donn√©es de prompts existent\n",
    "prompts_path = \"data/processed/sentiment/prompts.jsonl\"\n",
    "\n",
    "if not os.path.exists(prompts_path):\n",
    "    print(\" Fichier prompts.jsonl non trouv√©!\")\n",
    "    print(\"Ex√©cution de prepare_prompts.py...\")\n",
    "    !python scripts/prepare_prompts.py\n",
    "    print(\"Prompts pr√©par√©s\")\n",
    "else:\n",
    "    # Compter le nombre de prompts\n",
    "    with open(prompts_path, 'r') as f:\n",
    "        num_prompts = sum(1 for line in f if line.strip())\n",
    "    print(f\"{num_prompts} prompts trouv√©s dans {prompts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73358b",
   "metadata": {},
   "source": [
    "## 4. Configuration des Hyperparam√®tres\n",
    "\n",
    "Vous pouvez modifier ces param√®tres selon vos besoins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd47405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Configuration PPO:\n",
      "  Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  Batch size: 2\n",
      "  Learning rate: 1e-5\n",
      "  Epochs: 1\n",
      "\n",
      " Param√®tres PPO:\n",
      "  Clip epsilon: 0.2\n",
      "  Value coef: 0.5\n",
      "  Entropy coef: 0.01\n",
      "  Target KL: 0.01\n",
      "  PPO epochs: 4\n",
      "\n",
      " G√©n√©ration:\n",
      "  Max length: 128\n",
      "  Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Charger la config par d√©faut\n",
    "config_path = \"configs/ppo_sentiment.yaml\"\n",
    "config = load_yaml_config(config_path)\n",
    "\n",
    "# Afficher les param√®tres principaux\n",
    "print(\" Configuration PPO:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(\"\\n Param√®tres PPO:\")\n",
    "print(f\"  Clip epsilon: {config['ppo']['clip_epsilon']}\")\n",
    "print(f\"  Value coef: {config['ppo']['value_coef']}\")\n",
    "print(f\"  Entropy coef: {config['ppo']['entropy_coef']}\")\n",
    "print(f\"  Target KL: {config['ppo']['target_kl']}\")\n",
    "print(f\"  PPO epochs: {config['ppo']['num_ppo_epochs']}\")\n",
    "print(\"\\n G√©n√©ration:\")\n",
    "print(f\"  Max length: {config['generation']['max_length']}\")\n",
    "print(f\"  Temperature: {config['generation']['temperature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a3adf",
   "metadata": {},
   "source": [
    "### Modifier les param√®tres (optionnel)\n",
    "\n",
    "D√©commentez et modifiez si vous voulez changer certains param√®tres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec978b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Configuration personnalis√©e (si modifi√©e)\n"
     ]
    }
   ],
   "source": [
    "# Exemple de modifications\n",
    "# config['training']['batch_size'] = 1  # R√©duire si probl√®me de m√©moire\n",
    "# config['training']['num_epochs'] = 2  # Plus d'epochs\n",
    "# config['ppo']['num_ppo_epochs'] = 2   # Moins d'epochs PPO par batch\n",
    "# config['generation']['max_length'] = 64  # R√©ponses plus courtes\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration personnalis√©e (si modifi√©e)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d12637",
   "metadata": {},
   "source": [
    "## 5. Chargement des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eed1b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chargement des mod√®les...\n",
      "   Cela peut prendre quelques minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7b693549614a9eb5ddce842168d605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d7204c158d413abb62b590df7dc2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedbc302d79242c0b0ce2934672f4fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d849c1bb7547d694a26bff318c435f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd12594bc8544f9ab11695201de1e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765fbb9d7da5421d91395837fc5d9783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23094f3865f43b790cb8920ef1ef5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mod√®les charg√©s: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "   Policy model: 1,100,048,384 param√®tres\n",
      "   Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\" Chargement des mod√®les...\")\n",
    "print(\"   Cela peut prendre quelques minutes...\")\n",
    "\n",
    "model_name = config[\"model\"][\"name\"]\n",
    "dtype = config[\"model\"][\"dtype\"]\n",
    "\n",
    "# Charger les mod√®les (policy et r√©f√©rence)\n",
    "mb = load_models(model_name, dtype=dtype, device=device)\n",
    "tokenizer = mb.tokenizer\n",
    "\n",
    "print(f\" Mod√®les charg√©s: {model_name}\")\n",
    "print(f\"   Policy model: {mb.policy_model.num_parameters():,} param√®tres\")\n",
    "print(f\"   Device: {mb.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ead1b",
   "metadata": {},
   "source": [
    "## 6. Pr√©paration du DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9bb1cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset charg√©: 2497 prompts\n",
      " DataLoader cr√©√©: 1249 batches\n"
     ]
    }
   ],
   "source": [
    "# Charger le dataset de prompts\n",
    "prompt_dataset = PromptDataset(config[\"data\"][\"prompt_path\"])\n",
    "max_prompt_length = config[\"data\"][\"max_prompt_length\"]\n",
    "\n",
    "print(f\"Dataset charg√©: {len(prompt_dataset)} prompts\")\n",
    "\n",
    "# Fonction de collate\n",
    "def collate(batch):\n",
    "    return prompt_collate_fn(\n",
    "        batch,\n",
    "        tokenizer=tokenizer,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "    )\n",
    "\n",
    "# DataLoader\n",
    "prompt_loader = DataLoader(\n",
    "    prompt_dataset,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "print(f\" DataLoader cr√©√©: {len(prompt_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14557e",
   "metadata": {},
   "source": [
    "## 7. Initialisation du PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3323888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation du PPO Trainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1cec2f174424a55b85690e46a83d2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1398c3dc0634f5fafa328fcac7db697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364d4f4e567c4c27b8d51d72ce0bef23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef7658e50f74d0ab8547afcf927480b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialis√©\n",
      "   Reward model: distilbert-base-uncased-finetuned-sst-2-english\n",
      "   Save dir: checkpoints/sentiment_ppo\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialisation du PPO Trainer...\")\n",
    "\n",
    "# Cr√©er le trainer\n",
    "trainer = PPOTrainer(\n",
    "    model_bundle=mb,\n",
    "    prompt_loader=prompt_loader,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialis√©\")\n",
    "print(f\"   Reward model: {config['reward_model']['name']}\")\n",
    "print(f\"   Save dir: {config['logging']['save_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30aecff",
   "metadata": {},
   "source": [
    "## 8. Entra√Ænement PPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092fe4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " D√©marrage de l'entra√Ænement PPO\n",
      "   Exp√©rience: ppo_sentiment_tinyllama\n",
      "   Device: cuda\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded5728131e5468f90070d9f0e8b77b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PPO Epoch 1/1:   0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Erreur pendant l'entra√Ænement: Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-625790226.py\", line 13, in <cell line: 0>\n",
      "    trainer.train()\n",
      "  File \"/content/Direct-Preference-Optimization/src/ppo/ppo_trainer.py\", line 83, in train\n",
      "    stats = self._ppo_step(batch)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/Direct-Preference-Optimization/src/ppo/ppo_trainer.py\", line 130, in _ppo_step\n",
      "    old_values = self.policy_model.value_head(last_hidden)  # [B]\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/Direct-Preference-Optimization/src/dpo/reward_models.py\", line 118, in forward\n",
      "    values = self.linear(hidden_states).squeeze(-1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\" D√©marrage de l'entra√Ænement PPO\")\n",
    "print(f\"   Exp√©rience: {config['experiment_name']}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Lancer l'entra√Ænement\n",
    "    trainer.train()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Entra√Ænement termin√©!\")\n",
    "    print(f\"   Temps total: {elapsed/60:.2f} minutes\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n  Entra√Ænement interrompu par l'utilisateur\")\n",
    "    print(\"   Les checkpoints partiels ont √©t√© sauvegard√©s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n Erreur pendant l'entra√Ænement: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bcd2c",
   "metadata": {},
   "source": [
    "## 9. V√©rification des Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "save_dir = config['logging']['save_dir']\n",
    "checkpoints = glob.glob(os.path.join(save_dir, \"*.pt\"))\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"‚úÖ {len(checkpoints)} checkpoint(s) sauvegard√©(s):\")\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        size_mb = os.path.getsize(ckpt) / (1024**2)\n",
    "        print(f\"   üìÅ {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Aucun checkpoint trouv√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672af94",
   "metadata": {},
   "source": [
    "## 10. Test de G√©n√©ration Rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b26dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la g√©n√©ration avec le mod√®le entra√Æn√©\n",
    "test_prompts = [\n",
    "    \"The movie was\",\n",
    "    \"I think this product is\",\n",
    "    \"The customer service was\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Test de g√©n√©ration avec le mod√®le PPO:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mb.policy_model.eval()\n",
    "with torch.no_grad():\n",
    "    for prompt in test_prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = mb.policy_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\nüí¨ Prompt: {prompt}\")\n",
    "        print(f\"‚ú® Response: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcee02",
   "metadata": {},
   "source": [
    "## üéØ Prochaines √âtapes\n",
    "\n",
    "Maintenant que l'entra√Ænement est termin√©, vous pouvez:\n",
    "\n",
    "1. **√âvaluer le mod√®le** avec `eval_sentiment2.py`:\n",
    "   ```bash\n",
    "   python scripts/eval_sentiment2.py --method ppo --num_samples 3\n",
    "   ```\n",
    "\n",
    "2. **Comparer avec DPO et GRPO**:\n",
    "   ```bash\n",
    "   python scripts/compare_methods.py\n",
    "   ```\n",
    "\n",
    "3. **Visualiser les r√©sultats**:\n",
    "   ```bash\n",
    "   python scripts/visualize_comparison.py\n",
    "   ```\n",
    "\n",
    "4. **Modifier les hyperparam√®tres** et r√©entra√Æner pour comparer les performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc71b6e",
   "metadata": {},
   "source": [
    "## üìä Notes\n",
    "\n",
    "- Les checkpoints sont sauvegard√©s dans `checkpoints/sentiment_ppo/`\n",
    "- Le mod√®le PPO inclut un **value head** en plus de la policy\n",
    "- L'entra√Ænement utilise un **reward model** bas√© sur le sentiment\n",
    "- Le **target KL** permet l'early stopping pour la stabilit√©\n",
    "- Sur CPU, l'entra√Ænement est plus lent mais plus stable que sur MPS (macOS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
