{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f02387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a14e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Direct-Preference-Optimization'...\n",
      "remote: Enumerating objects: 160, done.\u001b[K\n",
      "remote: Counting objects: 100% (142/142), done.\u001b[K\n",
      "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
      "remote: Total 160 (delta 76), reused 105 (delta 50), pack-reused 18 (from 1)\u001b[K\n",
      "Receiving objects: 100% (160/160), 1.27 MiB | 17.10 MiB/s, done.\n",
      "Resolving deltas: 100% (76/76), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone -b jeremy --single-branch https://github.com/Jalalbaim/Direct-Preference-Optimization.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974e373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf Direct-Preference-Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f68927",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8451a6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Direct-Preference-Optimization\n",
      " R√©pertoire actuel: /content/Direct-Preference-Optimization\n",
      " Contenu: ['train_ppo.ipynb', 'configs', 'src', '.git', 'requirements.txt', '.gitignore', 'README.md', 'scripts', 'data']\n"
     ]
    }
   ],
   "source": [
    "# Changer de r√©pertoire vers le projet clon√©\n",
    "%cd Direct-Preference-Optimization/\n",
    "\n",
    "\n",
    "import os\n",
    "print(f\" R√©pertoire actuel: {os.getcwd()}\")\n",
    "print(f\" Contenu: {os.listdir('.')[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc78d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installation silencieuse des packages n√©cessaires\n",
    "!pip install torch transformers datasets accelerate sentencepiece protobuf tqdm pyyaml scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a7988",
   "metadata": {},
   "source": [
    "### √âtape 2: Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6d5863",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2500459688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144e3f9",
   "metadata": {},
   "source": [
    "## 1. Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de70e75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Working directory: /content/Direct-Preference-Optimization\n",
      "Imports r√©ussis\n",
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Ajouter le r√©pertoire racine au path (d√©j√† dans le bon r√©pertoire apr√®s %cd)\n",
    "ROOT = os.path.abspath('.')\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.append(ROOT)\n",
    "\n",
    "print(f\" Working directory: {os.getcwd()}\")\n",
    "\n",
    "from src.dpo.models import load_models\n",
    "from src.dpo.data import PromptDataset, prompt_collate_fn\n",
    "from src.ppo.ppo_trainer import PPOTrainer\n",
    "from src.dpo.utils import load_yaml_config\n",
    "\n",
    "print(\"Imports r√©ussis\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fc6cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497 prompts trouv√©s dans data/processed/sentiment/prompts.jsonl\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier si les donn√©es de prompts existent\n",
    "prompts_path = \"data/processed/sentiment/prompts.jsonl\"\n",
    "\n",
    "if not os.path.exists(prompts_path):\n",
    "    print(\" Fichier prompts.jsonl non trouv√©!\")\n",
    "    print(\"Ex√©cution de prepare_prompts.py...\")\n",
    "    !python scripts/prepare_prompts.py\n",
    "    print(\"Prompts pr√©par√©s\")\n",
    "else:\n",
    "    # Compter le nombre de prompts\n",
    "    with open(prompts_path, 'r') as f:\n",
    "        num_prompts = sum(1 for line in f if line.strip())\n",
    "    print(f\"{num_prompts} prompts trouv√©s dans {prompts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73358b",
   "metadata": {},
   "source": [
    "## 4. Configuration des Hyperparam√®tres\n",
    "\n",
    "Vous pouvez modifier ces param√®tres selon vos besoins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd47405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Configuration PPO:\n",
      "  Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  Batch size: 2\n",
      "  Learning rate: 1e-5\n",
      "  Epochs: 1\n",
      "\n",
      " Param√®tres PPO:\n",
      "  Clip epsilon: 0.2\n",
      "  Value coef: 0.5\n",
      "  Entropy coef: 0.01\n",
      "  Target KL: 0.01\n",
      "  PPO epochs: 4\n",
      "\n",
      " G√©n√©ration:\n",
      "  Max length: 128\n",
      "  Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Charger la config par d√©faut\n",
    "config_path = \"configs/ppo_sentiment.yaml\"\n",
    "config = load_yaml_config(config_path)\n",
    "\n",
    "# Afficher les param√®tres principaux\n",
    "print(\" Configuration PPO:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(\"\\n Param√®tres PPO:\")\n",
    "print(f\"  Clip epsilon: {config['ppo']['clip_epsilon']}\")\n",
    "print(f\"  Value coef: {config['ppo']['value_coef']}\")\n",
    "print(f\"  Entropy coef: {config['ppo']['entropy_coef']}\")\n",
    "print(f\"  Target KL: {config['ppo']['target_kl']}\")\n",
    "print(f\"  PPO epochs: {config['ppo']['num_ppo_epochs']}\")\n",
    "print(\"\\n G√©n√©ration:\")\n",
    "print(f\"  Max length: {config['generation']['max_length']}\")\n",
    "print(f\"  Temperature: {config['generation']['temperature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a3adf",
   "metadata": {},
   "source": [
    "### Modifier les param√®tres (optionnel)\n",
    "\n",
    "D√©commentez et modifiez si vous voulez changer certains param√®tres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec978b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Configuration personnalis√©e (si modifi√©e)\n"
     ]
    }
   ],
   "source": [
    "# Exemple de modifications\n",
    "# config['training']['batch_size'] = 1  # R√©duire si probl√®me de m√©moire\n",
    "# config['training']['num_epochs'] = 2  # Plus d'epochs\n",
    "# config['ppo']['num_ppo_epochs'] = 2   # Moins d'epochs PPO par batch\n",
    "# config['generation']['max_length'] = 64  # R√©ponses plus courtes\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration personnalis√©e (si modifi√©e)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d12637",
   "metadata": {},
   "source": [
    "## 5. Chargement des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4caa8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eed1b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mod√®les charg√©s: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "   Policy model: 1,100,048,384 param√®tres\n",
      "   Device: cuda\n",
      "\n",
      " Diagnostic device:\n",
      "   Policy: cuda:0\n",
      "   Ref: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = config[\"model\"][\"name\"]\n",
    "dtype = config[\"model\"][\"dtype\"]\n",
    "\n",
    "# Charger les mod√®les (policy et r√©f√©rence)\n",
    "mb = load_models(model_name, dtype=dtype, device=device)\n",
    "tokenizer = mb.tokenizer\n",
    "\n",
    "print(f\" Mod√®les charg√©s: {model_name}\")\n",
    "print(f\"   Policy model: {mb.policy_model.num_parameters():,} param√®tres\")\n",
    "print(f\"   Device: {mb.device}\")\n",
    "\n",
    "# üîç Diagnostic: V√©rifier que tout est sur GPU\n",
    "print(f\"\\n Diagnostic device:\")\n",
    "print(f\"   Policy: {next(mb.policy_model.parameters()).device}\")\n",
    "print(f\"   Ref: {next(mb.ref_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ead1b",
   "metadata": {},
   "source": [
    "## 6. Pr√©paration du DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9bb1cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset charg√©: 2497 prompts\n",
      " DataLoader cr√©√©: 1249 batches\n"
     ]
    }
   ],
   "source": [
    "# Charger le dataset de prompts\n",
    "prompt_dataset = PromptDataset(config[\"data\"][\"prompt_path\"])\n",
    "max_prompt_length = config[\"data\"][\"max_prompt_length\"]\n",
    "\n",
    "print(f\"Dataset charg√©: {len(prompt_dataset)} prompts\")\n",
    "\n",
    "# Fonction de collate\n",
    "def collate(batch):\n",
    "    return prompt_collate_fn(\n",
    "        batch,\n",
    "        tokenizer=tokenizer,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "    )\n",
    "\n",
    "# DataLoader\n",
    "prompt_loader = DataLoader(\n",
    "    prompt_dataset,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "print(f\" DataLoader cr√©√©: {len(prompt_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14557e",
   "metadata": {},
   "source": [
    "## 7. Initialisation du PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3323888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Initialisation du PPO Trainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f113760c0e8e45a49dd80ede39b5a980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e896638f1db4f1ba6b3696703bdea44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ebdc7a3c50402eb2b8f2b417803eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc40e1694364a6b84fdf71caaf4fdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7a37b0b82f41a4a8a244bee47a0db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebcce8aec274f538078f8a0251c6249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialis√©\n",
      "   Reward model: lvwerra/distilbert-imdb\n",
      "   Save dir: checkpoints/sentiment_ppo\n",
      "\n",
      "üîç Diagnostic final - Tous les composants:\n",
      "   Trainer device: cuda\n",
      "   Policy model: cuda:0\n",
      "   Value head: cuda:0\n",
      "   Ref model: cuda:0\n",
      "   Reward model: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"‚öôÔ∏è Initialisation du PPO Trainer...\")\n",
    "\n",
    "# Cr√©er le trainer\n",
    "trainer = PPOTrainer(\n",
    "    model_bundle=mb,\n",
    "    prompt_loader=prompt_loader,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialis√©\")\n",
    "print(f\"   Reward model: {config['reward_model']['name']}\")\n",
    "print(f\"   Save dir: {config['logging']['save_dir']}\")\n",
    "\n",
    "# üîç Diagnostic: V√©rifier que tout est sur le m√™me device\n",
    "print(f\"\\nüîç Diagnostic final - Tous les composants:\")\n",
    "print(f\"   Trainer device: {trainer.device}\")\n",
    "print(f\"   Policy model: {next(trainer.policy_model.parameters()).device}\")\n",
    "print(f\"   Value head: {next(trainer.policy_model.value_head.parameters()).device}\")\n",
    "print(f\"   Ref model: {next(trainer.ref_model.parameters()).device}\")\n",
    "print(f\"   Reward model: {trainer.reward_model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30aecff",
   "metadata": {},
   "source": [
    "## 8. Entra√Ænement PPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce8439df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Syst√®me de diagnostic activ√©\n",
      "   Chaque step PPO affichera les types/devices/shapes de tous les tenseurs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7b5a4f2e5f42e0b9fce5107d945d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üîç Syst√®me de diagnostic pour tracer les erreurs\n",
    "def debug_tensor(name, tensor, indent=0):\n",
    "    \"\"\"Affiche les infos d√©taill√©es d'un tenseur\"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    if tensor is None:\n",
    "        print(f\"{prefix}{name}: None\")\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        print(f\"{prefix}{name}: shape={tensor.shape}, dtype={tensor.dtype}, device={tensor.device}, requires_grad={tensor.requires_grad}\")\n",
    "    elif isinstance(tensor, (list, tuple)):\n",
    "        print(f\"{prefix}{name}: {type(tensor).__name__} of length {len(tensor)}\")\n",
    "        if len(tensor) > 0 and isinstance(tensor[0], torch.Tensor):\n",
    "            debug_tensor(f\"{name}[0]\", tensor[0], indent+1)\n",
    "    elif isinstance(tensor, dict):\n",
    "        print(f\"{prefix}{name}: dict with keys {list(tensor.keys())}\")\n",
    "        for k, v in tensor.items():\n",
    "            debug_tensor(f\"{name}['{k}']\", v, indent+1)\n",
    "    else:\n",
    "        print(f\"{prefix}{name}: {type(tensor).__name__} = {tensor}\")\n",
    "\n",
    "# Wrapper pour le _ppo_step avec diagnostic\n",
    "original_ppo_step = trainer._ppo_step\n",
    "\n",
    "def debug_ppo_step(batch):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç DEBUG PPO STEP - D√âBUT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n1Ô∏è‚É£ BATCH INPUT:\")\n",
    "        debug_tensor(\"batch\", batch)\n",
    "        \n",
    "        # V√©rifier que batch est d√©plac√© sur device\n",
    "        batch_on_device = {k: v.to(trainer.device) for k, v in batch.items()}\n",
    "        print(\"\\n2Ô∏è‚É£ BATCH APR√àS .to(device):\")\n",
    "        debug_tensor(\"batch_on_device\", batch_on_device)\n",
    "        \n",
    "        print(\"\\n3Ô∏è‚É£ G√âN√âRATION DES R√âPONSES...\")\n",
    "        trainer.policy_model.eval()\n",
    "        \n",
    "        # V√©rifier les inputs de g√©n√©ration\n",
    "        print(\"\\n  Inputs de g√©n√©ration:\")\n",
    "        debug_tensor(\"  prompt_ids\", batch_on_device[\"input_ids\"])\n",
    "        debug_tensor(\"  prompt_mask\", batch_on_device[\"attention_mask\"])\n",
    "        \n",
    "        # G√©n√©ration\n",
    "        generated_ids = trainer.policy_model.generate(\n",
    "            input_ids=batch_on_device[\"input_ids\"],\n",
    "            attention_mask=batch_on_device[\"attention_mask\"],\n",
    "            max_new_tokens=trainer.max_gen_length,\n",
    "            temperature=trainer.temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=trainer.mb.tokenizer.pad_token_id,\n",
    "        )\n",
    "        print(\"\\n  Outputs de g√©n√©ration:\")\n",
    "        debug_tensor(\"  generated_ids\", generated_ids)\n",
    "        \n",
    "        # Cr√©er response_mask\n",
    "        prompt_len = batch_on_device[\"input_ids\"].shape[1]\n",
    "        model_dtype = next(trainer.policy_model.parameters()).dtype\n",
    "        response_mask = torch.zeros_like(generated_ids, dtype=model_dtype)\n",
    "        response_mask[:, prompt_len:] = 1\n",
    "        \n",
    "        print(\"\\n  Masques cr√©√©s:\")\n",
    "        debug_tensor(\"  response_mask\", response_mask)\n",
    "        \n",
    "        # Calculer logits\n",
    "        full_mask = torch.ones_like(generated_ids, dtype=torch.long)\n",
    "        debug_tensor(\"  full_mask\", full_mask)\n",
    "        \n",
    "        print(\"\\n4Ô∏è‚É£ FORWARD POLICY MODEL...\")\n",
    "        policy_outputs = trainer.policy_model(\n",
    "            input_ids=generated_ids,\n",
    "            attention_mask=full_mask,\n",
    "        )\n",
    "        print(\"  Policy outputs:\")\n",
    "        debug_tensor(\"  logits\", policy_outputs.logits)\n",
    "        \n",
    "        print(\"\\n5Ô∏è‚É£ CALCUL DES LOG-PROBS...\")\n",
    "        shift_logits = policy_outputs.logits[:, :-1, :].contiguous()\n",
    "        shift_labels = generated_ids[:, 1:].contiguous()\n",
    "        shift_mask = response_mask[:, 1:].contiguous()\n",
    "        \n",
    "        print(\"  Tenseurs d√©cal√©s:\")\n",
    "        debug_tensor(\"  shift_logits\", shift_logits)\n",
    "        debug_tensor(\"  shift_labels\", shift_labels)\n",
    "        debug_tensor(\"  shift_mask\", shift_mask)\n",
    "        \n",
    "        log_probs = torch.log_softmax(shift_logits, dim=-1)\n",
    "        debug_tensor(\"  log_probs\", log_probs)\n",
    "        \n",
    "        token_logps = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        debug_tensor(\"  token_logps\", token_logps)\n",
    "        \n",
    "        token_logps_masked = token_logps * shift_mask\n",
    "        debug_tensor(\"  token_logps_masked\", token_logps_masked)\n",
    "        \n",
    "        seq_logps = token_logps_masked.sum(dim=-1)\n",
    "        debug_tensor(\"  seq_logps\", seq_logps)\n",
    "        \n",
    "        print(\"\\n6Ô∏è‚É£ CALCUL DES REWARDS...\")\n",
    "        texts = trainer.mb.tokenizer.batch_decode(generated_ids[:, prompt_len:], skip_special_tokens=True)\n",
    "        print(f\"  Textes g√©n√©r√©s: {texts}\")\n",
    "        \n",
    "        rewards = trainer.reward_model.compute_rewards(texts)\n",
    "        debug_tensor(\"  rewards (avant conversion)\", rewards)\n",
    "        \n",
    "        rewards = rewards.to(dtype=model_dtype)\n",
    "        debug_tensor(\"  rewards (apr√®s conversion)\", rewards)\n",
    "        \n",
    "        print(\"\\n7Ô∏è‚É£ FORWARD AVEC HIDDEN STATES...\")\n",
    "        old_outputs = trainer.policy_model(\n",
    "            input_ids=generated_ids,\n",
    "            attention_mask=full_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        print(\"  Hidden states:\")\n",
    "        if hasattr(old_outputs, 'hidden_states') and old_outputs.hidden_states:\n",
    "            debug_tensor(\"  hidden_states[-1]\", old_outputs.hidden_states[-1])\n",
    "            last_hidden = old_outputs.hidden_states[-1][:, -1, :]\n",
    "            debug_tensor(\"  last_hidden\", last_hidden)\n",
    "            \n",
    "            print(\"\\n8Ô∏è‚É£ VALUE HEAD...\")\n",
    "            values = trainer.policy_model.value_head(last_hidden)\n",
    "            debug_tensor(\"  values\", values)\n",
    "            \n",
    "            print(\"\\n9Ô∏è‚É£ ADVANTAGES...\")\n",
    "            advantages = rewards - values\n",
    "            debug_tensor(\"  advantages\", advantages)\n",
    "        \n",
    "        print(\"\\n‚úÖ APPEL DE LA FONCTION ORIGINALE...\")\n",
    "        trainer.policy_model.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"‚ùå ERREUR D√âTECT√âE: {type(e).__name__}\")\n",
    "        print(f\"   Message: {e}\")\n",
    "        print(\"=\"*80)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    # Appeler la fonction originale\n",
    "    return original_ppo_step(batch_on_device)\n",
    "\n",
    "# Remplacer temporairement _ppo_step par la version debug\n",
    "trainer._ppo_step = debug_ppo_step\n",
    "\n",
    "print(\"‚úÖ Syst√®me de diagnostic activ√©\")\n",
    "print(\"   Chaque step PPO affichera les types/devices/shapes de tous les tenseurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092fe4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT PPO (AVEC DIAGNOSTIC)\n",
      "   Exp√©rience: ppo_sentiment_tinyllama\n",
      "   Device: cuda\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce7c2afb95040898fc04aa9099c8b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PPO Epoch 1/1:   0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç DEBUG PPO STEP - D√âBUT\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ BATCH INPUT:\n",
      "batch: dict with keys ['input_ids', 'attention_mask']\n",
      "  batch['input_ids']: shape=torch.Size([2, 76]), dtype=torch.int64, device=cpu, requires_grad=False\n",
      "  batch['attention_mask']: shape=torch.Size([2, 76]), dtype=torch.int64, device=cpu, requires_grad=False\n",
      "\n",
      "2Ô∏è‚É£ BATCH APR√àS .to(device):\n",
      "batch_on_device: dict with keys ['input_ids', 'attention_mask']\n",
      "  batch_on_device['input_ids']: shape=torch.Size([2, 76]), dtype=torch.int64, device=cuda:0, requires_grad=False\n",
      "  batch_on_device['attention_mask']: shape=torch.Size([2, 76]), dtype=torch.int64, device=cuda:0, requires_grad=False\n",
      "\n",
      "3Ô∏è‚É£ G√âN√âRATION DES R√âPONSES...\n",
      "\n",
      "  Inputs de g√©n√©ration:\n",
      "  prompt_ids: shape=torch.Size([2, 76]), dtype=torch.int64, device=cuda:0, requires_grad=False\n",
      "  prompt_mask: shape=torch.Size([2, 76]), dtype=torch.int64, device=cuda:0, requires_grad=False\n",
      "\n",
      "  Outputs de g√©n√©ration:\n",
      "  generated_ids: shape=torch.Size([2, 204]), dtype=torch.int64, device=cuda:0, requires_grad=False\n",
      "\n",
      "  Masques cr√©√©s:\n",
      "  response_mask: shape=torch.Size([2, 204]), dtype=torch.bfloat16, device=cuda:0, requires_grad=False\n",
      "  full_mask: shape=torch.Size([2, 204]), dtype=torch.int64, device=cuda:0, requires_grad=False\n",
      "\n",
      "4Ô∏è‚É£ FORWARD POLICY MODEL...\n",
      "  Policy outputs:\n",
      "  logits: shape=torch.Size([2, 204, 32000]), dtype=torch.bfloat16, device=cuda:0, requires_grad=True\n",
      "\n",
      "5Ô∏è‚É£ CALCUL DES LOG-PROBS...\n",
      "  Tenseurs d√©cal√©s:\n",
      "  shift_logits: shape=torch.Size([2, 203, 32000]), dtype=torch.bfloat16, device=cuda:0, requires_grad=True\n",
      "  shift_labels: shape=torch.Size([2, 203]), dtype=torch.int64, device=cuda:0, requires_grad=False\n",
      "  shift_mask: shape=torch.Size([2, 203]), dtype=torch.bfloat16, device=cuda:0, requires_grad=False\n",
      "  log_probs: shape=torch.Size([2, 203, 32000]), dtype=torch.bfloat16, device=cuda:0, requires_grad=True\n",
      "  token_logps: shape=torch.Size([2, 203]), dtype=torch.bfloat16, device=cuda:0, requires_grad=True\n",
      "  token_logps_masked: shape=torch.Size([2, 203]), dtype=torch.bfloat16, device=cuda:0, requires_grad=True\n",
      "  seq_logps: shape=torch.Size([2]), dtype=torch.bfloat16, device=cuda:0, requires_grad=True\n",
      "\n",
      "6Ô∏è‚É£ CALCUL DES REWARDS...\n",
      "  Textes g√©n√©r√©s: ['the gangs that threaten their neighbourhood. The \\'breakin\\' routine is pretty basic and the soundtrack is pretty generic but who cares? This is what the \\'breakin\\' craze was all about in the early \\'80s and this movie is a great example of why.\\nIt\\'s not just the music that makes \"Breakin\\'\" a great movie. It\\'s also the cinematography, which is a real highlight. Director Rita Racine gives us some lovely close-ups of the dancing and lots of great use of the camera to show us how the characters move, and the', \"\\nNew York, I Love You, or rather should-be-titled Manhattan, I Love Looking At Your People In Sometimes Love, is a precise example of the difference between telling a story and telling a situation. Case in point, look at two of the segments in the film, one where Ethan Hawke lights a cigarette for a little while, then puts it out, and the second where he takes a bite of a sandwich. The former is a simple, almost fleeting gesture that adds nothing to the story, while the latter is a crucial moment that underscores Hawke's character\"]\n",
      "  rewards (avant conversion): shape=torch.Size([2]), dtype=torch.float32, device=cuda:0, requires_grad=False\n",
      "  rewards (apr√®s conversion): shape=torch.Size([2]), dtype=torch.bfloat16, device=cuda:0, requires_grad=False\n",
      "\n",
      "7Ô∏è‚É£ FORWARD AVEC HIDDEN STATES...\n",
      "  Hidden states:\n",
      "  hidden_states[-1]: shape=torch.Size([2, 204, 2048]), dtype=torch.bfloat16, device=cuda:0, requires_grad=True\n",
      "  last_hidden: shape=torch.Size([2, 2048]), dtype=torch.bfloat16, device=cuda:0, requires_grad=True\n",
      "\n",
      "8Ô∏è‚É£ VALUE HEAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  values: shape=torch.Size([2]), dtype=torch.bfloat16, device=cuda:0, requires_grad=True\n",
      "\n",
      "9Ô∏è‚É£ ADVANTAGES...\n",
      "  advantages: shape=torch.Size([2]), dtype=torch.bfloat16, device=cuda:0, requires_grad=True\n",
      "\n",
      "‚úÖ APPEL DE LA FONCTION ORIGINALE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚ùå ERREUR FINALE: OutOfMemoryError\n",
      "   Message: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 56887 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 430.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "================================================================================\n",
      "\n",
      "üí° Les d√©tails de l'erreur sont affich√©s ci-dessus dans les logs de debug\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-3226112305.py\", line 13, in <cell line: 0>\n",
      "    trainer.train()\n",
      "  File \"/content/Direct-Preference-Optimization/src/ppo/ppo_trainer.py\", line 85, in train\n",
      "    stats = self._ppo_step(batch)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipython-input-145877373.py\", line 143, in debug_ppo_step\n",
      "    return original_ppo_step(batch_on_device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/Direct-Preference-Optimization/src/ppo/ppo_trainer.py\", line 210, in _ppo_step\n",
      "    self.optimizer.step()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 82, in _use_grad\n",
      "    ret = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\", line 247, in step\n",
      "    adam(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 150, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\", line 953, in adam\n",
      "    func(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\", line 777, in _multi_tensor_adam\n",
      "    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 56887 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 430.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT PPO (AVEC DIAGNOSTIC)\")\n",
    "print(f\"   Exp√©rience: {config['experiment_name']}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Lancer l'entra√Ænement avec diagnostic\n",
    "    trainer.train()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚úÖ Entra√Ænement termin√©!\")\n",
    "    print(f\"   Temps total: {elapsed/60:.2f} minutes\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è  Entra√Ænement interrompu par l'utilisateur\")\n",
    "    print(\"   Les checkpoints partiels ont √©t√© sauvegard√©s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"‚ùå ERREUR FINALE: {type(e).__name__}\")\n",
    "    print(f\"   Message: {e}\")\n",
    "    print(\"=\"*80)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nüí° Les d√©tails de l'erreur sont affich√©s ci-dessus dans les logs de debug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac426d",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è FIX APPLIQU√â - RED√âMARRER LE KERNEL\n",
    "\n",
    "**Probl√®me identifi√© :** Le `ValueHead` √©tait cr√©√© avec `dtype=torch.bfloat16` en dur, ce qui causait un mismatch avec le mod√®le.\n",
    "\n",
    "**Solution :** Le code a √©t√© corrig√© dans [src/dpo/reward_models.py](src/dpo/reward_models.py).\n",
    "\n",
    "**Action requise :** \n",
    "1. ‚ö° **Red√©marrer le kernel** (pour recharger le code Python modifi√©)\n",
    "2. üîÑ **R√©ex√©cuter les cellules** depuis le d√©but (Setup, Imports, Config, Mod√®les, Trainer)\n",
    "3. ‚ñ∂Ô∏è **Relancer l'entra√Ænement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bcd2c",
   "metadata": {},
   "source": [
    "## 9. V√©rification des Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "save_dir = config['logging']['save_dir']\n",
    "checkpoints = glob.glob(os.path.join(save_dir, \"*.pt\"))\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"‚úÖ {len(checkpoints)} checkpoint(s) sauvegard√©(s):\")\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        size_mb = os.path.getsize(ckpt) / (1024**2)\n",
    "        print(f\"   üìÅ {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Aucun checkpoint trouv√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672af94",
   "metadata": {},
   "source": [
    "## 10. Test de G√©n√©ration Rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b26dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la g√©n√©ration avec le mod√®le entra√Æn√©\n",
    "test_prompts = [\n",
    "    \"The movie was\",\n",
    "    \"I think this product is\",\n",
    "    \"The customer service was\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Test de g√©n√©ration avec le mod√®le PPO:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mb.policy_model.eval()\n",
    "with torch.no_grad():\n",
    "    for prompt in test_prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = mb.policy_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\nüí¨ Prompt: {prompt}\")\n",
    "        print(f\"‚ú® Response: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcee02",
   "metadata": {},
   "source": [
    "## üéØ Prochaines √âtapes\n",
    "\n",
    "Maintenant que l'entra√Ænement est termin√©, vous pouvez:\n",
    "\n",
    "1. **√âvaluer le mod√®le** avec `eval_sentiment2.py`:\n",
    "   ```bash\n",
    "   python scripts/eval_sentiment2.py --method ppo --num_samples 3\n",
    "   ```\n",
    "\n",
    "2. **Comparer avec DPO et GRPO**:\n",
    "   ```bash\n",
    "   python scripts/compare_methods.py\n",
    "   ```\n",
    "\n",
    "3. **Visualiser les r√©sultats**:\n",
    "   ```bash\n",
    "   python scripts/visualize_comparison.py\n",
    "   ```\n",
    "\n",
    "4. **Modifier les hyperparam√®tres** et r√©entra√Æner pour comparer les performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc71b6e",
   "metadata": {},
   "source": [
    "## üìä Notes\n",
    "\n",
    "- Les checkpoints sont sauvegard√©s dans `checkpoints/sentiment_ppo/`\n",
    "- Le mod√®le PPO inclut un **value head** en plus de la policy\n",
    "- L'entra√Ænement utilise un **reward model** bas√© sur le sentiment\n",
    "- Le **target KL** permet l'early stopping pour la stabilit√©\n",
    "- Sur CPU, l'entra√Ænement est plus lent mais plus stable que sur MPS (macOS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
