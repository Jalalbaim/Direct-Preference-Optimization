{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a14e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Direct-Preference-Optimization'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
      "remote: Total 81 (delta 22), reused 48 (delta 17), pack-reused 18 (from 1)\u001b[K\n",
      "Receiving objects: 100% (81/81), 1.25 MiB | 17.96 MiB/s, done.\n",
      "Resolving deltas: 100% (22/22), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone -b jeremy --single-branch https://github.com/Jalalbaim/Direct-Preference-Optimization.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76da17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f001b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8451a6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Direct-Preference-Optimization/'\n",
      "/content/Direct-Preference-Optimization\n",
      " R√©pertoire actuel: /content/Direct-Preference-Optimization\n",
      " Contenu: ['.git', 'scripts', '.gitignore', 'src', 'README.md', 'data', 'train_ppo.ipynb', 'requirements.txt', 'configs']\n"
     ]
    }
   ],
   "source": [
    "# Changer de r√©pertoire vers le projet clon√©\n",
    "%cd Direct-Preference-Optimization/\n",
    "\n",
    "import os\n",
    "print(f\" R√©pertoire actuel: {os.getcwd()}\")\n",
    "print(f\" Contenu: {os.listdir('.')[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc78d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installation silencieuse des packages n√©cessaires\n",
    "!pip install torch transformers datasets accelerate sentencepiece protobuf tqdm pyyaml scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a7988",
   "metadata": {},
   "source": [
    "### √âtape 2: Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6d5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144e3f9",
   "metadata": {},
   "source": [
    "## 1. Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de70e75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Working directory: /content/Direct-Preference-Optimization\n",
      "Imports r√©ussis\n",
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Ajouter le r√©pertoire racine au path (d√©j√† dans le bon r√©pertoire apr√®s %cd)\n",
    "ROOT = os.path.abspath('.')\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.append(ROOT)\n",
    "\n",
    "print(f\" Working directory: {os.getcwd()}\")\n",
    "\n",
    "from src.dpo.models import load_models\n",
    "from src.dpo.data import PromptDataset, prompt_collate_fn\n",
    "from src.ppo.ppo_trainer import PPOTrainer\n",
    "from src.dpo.utils import load_yaml_config\n",
    "\n",
    "print(\"Imports r√©ussis\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9ed0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device s√©lectionn√©: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "   \n",
    "\n",
    "print(f\"Device s√©lectionn√©: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d664eb",
   "metadata": {},
   "source": [
    "## 3. V√©rification et Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fc6cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497 prompts trouv√©s dans data/processed/sentiment/prompts.jsonl\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier si les donn√©es de prompts existent\n",
    "prompts_path = \"data/processed/sentiment/prompts.jsonl\"\n",
    "\n",
    "if not os.path.exists(prompts_path):\n",
    "    print(\" Fichier prompts.jsonl non trouv√©!\")\n",
    "    print(\"Ex√©cution de prepare_prompts.py...\")\n",
    "    !python scripts/prepare_prompts.py\n",
    "    print(\"Prompts pr√©par√©s\")\n",
    "else:\n",
    "    # Compter le nombre de prompts\n",
    "    with open(prompts_path, 'r') as f:\n",
    "        num_prompts = sum(1 for line in f if line.strip())\n",
    "    print(f\"{num_prompts} prompts trouv√©s dans {prompts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73358b",
   "metadata": {},
   "source": [
    "## 4. Configuration des Hyperparam√®tres\n",
    "\n",
    "Vous pouvez modifier ces param√®tres selon vos besoins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd47405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Configuration PPO:\n",
      "  Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  Batch size: 2\n",
      "  Learning rate: 1e-5\n",
      "  Epochs: 1\n",
      "\n",
      " Param√®tres PPO:\n",
      "  Clip epsilon: 0.2\n",
      "  Value coef: 0.5\n",
      "  Entropy coef: 0.01\n",
      "  Target KL: 0.01\n",
      "  PPO epochs: 4\n",
      "\n",
      " G√©n√©ration:\n",
      "  Max length: 128\n",
      "  Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Charger la config par d√©faut\n",
    "config_path = \"configs/ppo_sentiment.yaml\"\n",
    "config = load_yaml_config(config_path)\n",
    "\n",
    "# Afficher les param√®tres principaux\n",
    "print(\" Configuration PPO:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(\"\\n Param√®tres PPO:\")\n",
    "print(f\"  Clip epsilon: {config['ppo']['clip_epsilon']}\")\n",
    "print(f\"  Value coef: {config['ppo']['value_coef']}\")\n",
    "print(f\"  Entropy coef: {config['ppo']['entropy_coef']}\")\n",
    "print(f\"  Target KL: {config['ppo']['target_kl']}\")\n",
    "print(f\"  PPO epochs: {config['ppo']['num_ppo_epochs']}\")\n",
    "print(\"\\n G√©n√©ration:\")\n",
    "print(f\"  Max length: {config['generation']['max_length']}\")\n",
    "print(f\"  Temperature: {config['generation']['temperature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a3adf",
   "metadata": {},
   "source": [
    "### Modifier les param√®tres (optionnel)\n",
    "\n",
    "D√©commentez et modifiez si vous voulez changer certains param√®tres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec978b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Configuration personnalis√©e (si modifi√©e)\n"
     ]
    }
   ],
   "source": [
    "# Exemple de modifications\n",
    "# config['training']['batch_size'] = 1  # R√©duire si probl√®me de m√©moire\n",
    "# config['training']['num_epochs'] = 2  # Plus d'epochs\n",
    "# config['ppo']['num_ppo_epochs'] = 2   # Moins d'epochs PPO par batch\n",
    "# config['generation']['max_length'] = 64  # R√©ponses plus courtes\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration personnalis√©e (si modifi√©e)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d12637",
   "metadata": {},
   "source": [
    "## 5. Chargement des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed1b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chargement des mod√®les...\n",
      "   Cela peut prendre quelques minutes...\n",
      " Mod√®les charg√©s: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "   Policy model: 1,100,048,384 param√®tres\n",
      "   Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"üì¶ Chargement des mod√®les...\")\n",
    "print(\"   Cela peut prendre quelques minutes...\")\n",
    "\n",
    "model_name = config[\"model\"][\"name\"]\n",
    "dtype = config[\"model\"][\"dtype\"]\n",
    "\n",
    "# Charger les mod√®les (policy et r√©f√©rence)\n",
    "mb = load_models(model_name, dtype=dtype, device=device)\n",
    "tokenizer = mb.tokenizer\n",
    "\n",
    "print(f\" Mod√®les charg√©s: {model_name}\")\n",
    "print(f\"   Policy model: {mb.policy_model.num_parameters():,} param√®tres\")\n",
    "print(f\"   Device: {mb.device}\")\n",
    "\n",
    "# üîç Diagnostic: V√©rifier que tout est sur GPU\n",
    "print(f\"\\nüîç Diagnostic device:\")\n",
    "print(f\"   Policy: {next(mb.policy_model.parameters()).device}\")\n",
    "print(f\"   Ref: {next(mb.ref_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ead1b",
   "metadata": {},
   "source": [
    "## 6. Pr√©paration du DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9bb1cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset charg√©: 2497 prompts\n",
      " DataLoader cr√©√©: 1249 batches\n"
     ]
    }
   ],
   "source": [
    "# Charger le dataset de prompts\n",
    "prompt_dataset = PromptDataset(config[\"data\"][\"prompt_path\"])\n",
    "max_prompt_length = config[\"data\"][\"max_prompt_length\"]\n",
    "\n",
    "print(f\"Dataset charg√©: {len(prompt_dataset)} prompts\")\n",
    "\n",
    "# Fonction de collate\n",
    "def collate(batch):\n",
    "    return prompt_collate_fn(\n",
    "        batch,\n",
    "        tokenizer=tokenizer,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "    )\n",
    "\n",
    "# DataLoader\n",
    "prompt_loader = DataLoader(\n",
    "    prompt_dataset,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "print(f\" DataLoader cr√©√©: {len(prompt_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14557e",
   "metadata": {},
   "source": [
    "## 7. Initialisation du PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation du PPO Trainer...\n",
      "‚úÖ Trainer initialis√©\n",
      "   Reward model: distilbert-base-uncased-finetuned-sst-2-english\n",
      "   Save dir: checkpoints/sentiment_ppo\n"
     ]
    }
   ],
   "source": [
    "print(\"‚öôÔ∏è Initialisation du PPO Trainer...\")\n",
    "\n",
    "# Cr√©er le trainer\n",
    "trainer = PPOTrainer(\n",
    "    model_bundle=mb,\n",
    "    prompt_loader=prompt_loader,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialis√©\")\n",
    "print(f\"   Reward model: {config['reward_model']['name']}\")\n",
    "print(f\"   Save dir: {config['logging']['save_dir']}\")\n",
    "\n",
    "# üîç Diagnostic: V√©rifier que tout est sur le m√™me device\n",
    "print(f\"\\nüîç Diagnostic final - Tous les composants:\")\n",
    "print(f\"   Trainer device: {trainer.device}\")\n",
    "print(f\"   Policy model: {next(trainer.policy_model.parameters()).device}\")\n",
    "print(f\"   Value head: {next(trainer.policy_model.value_head.parameters()).device}\")\n",
    "print(f\"   Ref model: {next(trainer.ref_model.parameters()).device}\")\n",
    "print(f\"   Reward model: {trainer.reward_model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30aecff",
   "metadata": {},
   "source": [
    "## 8. Entra√Ænement PPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "092fe4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " D√©marrage de l'entra√Ænement PPO\n",
      "   Exp√©rience: ppo_sentiment_tinyllama\n",
      "   Device: cuda\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8f0b9526744846828109c2ae42325e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PPO Epoch 1/1:   0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Erreur pendant l'entra√Ænement: Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-625790226.py\", line 13, in <cell line: 0>\n",
      "    trainer.train()\n",
      "  File \"/content/Direct-Preference-Optimization/src/ppo/ppo_trainer.py\", line 83, in train\n",
      "    stats = self._ppo_step(batch)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/Direct-Preference-Optimization/src/ppo/ppo_trainer.py\", line 130, in _ppo_step\n",
      "    old_values = self.policy_model.value_head(last_hidden)  # [B]\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/Direct-Preference-Optimization/src/dpo/reward_models.py\", line 118, in forward\n",
      "    values = self.linear(hidden_states).squeeze(-1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\" D√©marrage de l'entra√Ænement PPO\")\n",
    "print(f\"   Exp√©rience: {config['experiment_name']}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Lancer l'entra√Ænement\n",
    "    trainer.train()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Entra√Ænement termin√©!\")\n",
    "    print(f\"   Temps total: {elapsed/60:.2f} minutes\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n  Entra√Ænement interrompu par l'utilisateur\")\n",
    "    print(\"   Les checkpoints partiels ont √©t√© sauvegard√©s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n Erreur pendant l'entra√Ænement: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bcd2c",
   "metadata": {},
   "source": [
    "## 9. V√©rification des Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "save_dir = config['logging']['save_dir']\n",
    "checkpoints = glob.glob(os.path.join(save_dir, \"*.pt\"))\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"‚úÖ {len(checkpoints)} checkpoint(s) sauvegard√©(s):\")\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        size_mb = os.path.getsize(ckpt) / (1024**2)\n",
    "        print(f\"   üìÅ {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Aucun checkpoint trouv√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672af94",
   "metadata": {},
   "source": [
    "## 10. Test de G√©n√©ration Rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b26dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la g√©n√©ration avec le mod√®le entra√Æn√©\n",
    "test_prompts = [\n",
    "    \"The movie was\",\n",
    "    \"I think this product is\",\n",
    "    \"The customer service was\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Test de g√©n√©ration avec le mod√®le PPO:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mb.policy_model.eval()\n",
    "with torch.no_grad():\n",
    "    for prompt in test_prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = mb.policy_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=32,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\nüí¨ Prompt: {prompt}\")\n",
    "        print(f\"‚ú® Response: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcee02",
   "metadata": {},
   "source": [
    "## üéØ Prochaines √âtapes\n",
    "\n",
    "Maintenant que l'entra√Ænement est termin√©, vous pouvez:\n",
    "\n",
    "1. **√âvaluer le mod√®le** avec `eval_sentiment2.py`:\n",
    "   ```bash\n",
    "   python scripts/eval_sentiment2.py --method ppo --num_samples 3\n",
    "   ```\n",
    "\n",
    "2. **Comparer avec DPO et GRPO**:\n",
    "   ```bash\n",
    "   python scripts/compare_methods.py\n",
    "   ```\n",
    "\n",
    "3. **Visualiser les r√©sultats**:\n",
    "   ```bash\n",
    "   python scripts/visualize_comparison.py\n",
    "   ```\n",
    "\n",
    "4. **Modifier les hyperparam√®tres** et r√©entra√Æner pour comparer les performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc71b6e",
   "metadata": {},
   "source": [
    "## üìä Notes\n",
    "\n",
    "- Les checkpoints sont sauvegard√©s dans `checkpoints/sentiment_ppo/`\n",
    "- Le mod√®le PPO inclut un **value head** en plus de la policy\n",
    "- L'entra√Ænement utilise un **reward model** bas√© sur le sentiment\n",
    "- Le **target KL** permet l'early stopping pour la stabilit√©\n",
    "- Sur CPU, l'entra√Ænement est plus lent mais plus stable que sur MPS (macOS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
