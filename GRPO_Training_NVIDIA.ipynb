{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8887dd83",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation pour Colab (Python 3.12+)\n",
    "!pip install --upgrade pip setuptools wheel -q\n",
    "!pip install transformers[torch] datasets trl wandb accelerate -q\n",
    "\n",
    "# V√©rification\n",
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "print(\"‚úÖ Installation r√©ussie!\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"TRL: {trl.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb255f4b",
   "metadata": {},
   "source": [
    "## 0. Mount Google Drive (Optional - for Colab/Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c03897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive pour √©conomiser temps et quota\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    SAVE_BASE_PATH = '/content/drive/MyDrive/dpo_ppo_training'\n",
    "    os.makedirs(SAVE_BASE_PATH, exist_ok=True)\n",
    "    print(f\"‚úÖ Google Drive mont√©. Mod√®les sauvegard√©s sur: {SAVE_BASE_PATH}\")\n",
    "    USE_DRIVE = True\n",
    "except ImportError:\n",
    "    # Pas sur Colab\n",
    "    SAVE_BASE_PATH = './results'\n",
    "    USE_DRIVE = False\n",
    "    print(f\"‚ö†Ô∏è  Pas de Google Drive d√©tect√©. Stockage local: {SAVE_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b50ab9",
   "metadata": {},
   "source": [
    "## 1. Configuration GRPO\n",
    "\n",
    "**Param√®tres cl√©s GRPO :**\n",
    "- `num_sample_generations` : Nombre de g√©n√©rations par prompt (4-8 typique)\n",
    "- `USE_REWARD_MODEL` : False = classifier direct, True = reward model entra√Æn√©\n",
    "- `batch_size` : Nombre de prompts par batch\n",
    "- `learning_rate` : Taux d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babcc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# =====================================\n",
    "# CONFIGURATION PRINCIPALE\n",
    "# =====================================\n",
    "\n",
    "# GRPO Hyperparameters\n",
    "NUM_SAMPLE_GENERATIONS = 4  # G√©n√©rations par prompt (4-8 recommand√©)\n",
    "USE_REWARD_MODEL = False  # False: classifier direct, True: reward model entra√Æn√©\n",
    "\n",
    "BATCH_SIZE = 32  # Nombre de prompts par batch (ajuster selon GPU T4)\n",
    "MINI_BATCH_SIZE = 8  # Mini-batch pour GRPO optimization\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 1\n",
    "MAX_NEW_TOKENS = 24  # Tokens √† g√©n√©rer par completion\n",
    "\n",
    "# Model paths\n",
    "SFT_MODEL_PATH = f\"{SAVE_BASE_PATH}/sft_model\"\n",
    "REWARD_MODEL_PATH = f\"{SAVE_BASE_PATH}/reward_model\"  # Si USE_REWARD_MODEL=True\n",
    "GRPO_MODEL_PATH = f\"{SAVE_BASE_PATH}/grpo_model\"\n",
    "\n",
    "# Dataset\n",
    "PREFERENCE_PAIRS_PATH = f\"{SAVE_BASE_PATH}/datasets/preference_pairs.json\"\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"GRPO Configuration\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Num Sample Generations: {NUM_SAMPLE_GENERATIONS}\")\n",
    "print(f\"Use Reward Model: {USE_REWARD_MODEL}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Mini Batch Size: {MINI_BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Max New Tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"SFT Model: {SFT_MODEL_PATH}\")\n",
    "print(f\"GRPO Model Output: {GRPO_MODEL_PATH}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0559c4e",
   "metadata": {},
   "source": [
    "## 2. Load Dataset (Prompts Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ca50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 1: Chargement du dataset de prompts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load preference pairs to extract unique prompts\n",
    "if not os.path.exists(PREFERENCE_PAIRS_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Dataset introuvable: {PREFERENCE_PAIRS_PATH}\\n\"\n",
    "        \"Veuillez d'abord g√©n√©rer les paires de pr√©f√©rences (SFT notebook, cell 15).\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nüì• Chargement des prompts depuis: {PREFERENCE_PAIRS_PATH}\")\n",
    "with open(PREFERENCE_PAIRS_PATH, 'r', encoding='utf-8') as f:\n",
    "    preference_pairs = json.load(f)\n",
    "\n",
    "# Extract unique prompts\n",
    "unique_prompts = list(set([pair[\"prompt\"] for pair in preference_pairs]))\n",
    "print(f\"‚úÖ {len(unique_prompts)} prompts uniques extraits\")\n",
    "\n",
    "# Display samples\n",
    "print(f\"\\nExemples de prompts:\")\n",
    "for i in range(min(3, len(unique_prompts))):\n",
    "    print(f\"  {i+1}. {unique_prompts[i][:80]}...\")\n",
    "\n",
    "# Create dataset\n",
    "grpo_dataset = Dataset.from_dict({\n",
    "    \"query\": unique_prompts,\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset GRPO cr√©√© avec {len(grpo_dataset)} prompts\")\n",
    "print(f\"   Colonnes: {grpo_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf7c08",
   "metadata": {},
   "source": [
    "## 3. Load Models (Policy, Reference, Reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 2: Chargement des mod√®les\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ===== 1. Load Policy Model (SFT - will be trained) =====\n",
    "print(f\"\\nüì• Chargement du policy model (SFT)...\")\n",
    "if not os.path.exists(SFT_MODEL_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Mod√®le SFT introuvable: {SFT_MODEL_PATH}\\n\"\n",
    "        \"Veuillez d'abord entra√Æner le mod√®le SFT.\"\n",
    "    )\n",
    "\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "policy_model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"‚úÖ Policy model charg√© ({policy_model.num_parameters() / 1e9:.2f}B params)\")\n",
    "\n",
    "# ===== 2. Load Reference Model (Frozen SFT) =====\n",
    "print(f\"\\nüì• Chargement du reference model (SFT frozen)...\")\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "ref_model.to(device)\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "ref_model.eval()\n",
    "print(f\"‚úÖ Reference model charg√© (frozen)\")\n",
    "\n",
    "# ===== 3. Load Reward Model or Classifier =====\n",
    "print(f\"\\nüì• Chargement du reward model...\")\n",
    "\n",
    "if USE_REWARD_MODEL:\n",
    "    # Option A: Trained reward model\n",
    "    if not os.path.exists(REWARD_MODEL_PATH):\n",
    "        print(f\"‚ö†Ô∏è  USE_REWARD_MODEL=True mais mod√®le introuvable: {REWARD_MODEL_PATH}\")\n",
    "        print(f\"‚ö†Ô∏è  Basculement vers le classifier direct (siebert)\")\n",
    "        USE_REWARD_MODEL = False\n",
    "    else:\n",
    "        reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            REWARD_MODEL_PATH,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        reward_model.to(device)\n",
    "        reward_model.eval()\n",
    "        reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_PATH)\n",
    "        print(f\"‚úÖ Reward model charg√© depuis: {REWARD_MODEL_PATH}\")\n",
    "\n",
    "if not USE_REWARD_MODEL:\n",
    "    # Option B: Direct sentiment classifier (siebert)\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"siebert/sentiment-roberta-large-english\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    reward_model.to(device)\n",
    "    reward_model.eval()\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "    print(f\"‚úÖ Classifier siebert charg√© (labels: NEGATIVE=0, POSITIVE=1)\")\n",
    "\n",
    "# Freeze reward model\n",
    "for param in reward_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Tous les mod√®les charg√©s avec succ√®s\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f25e4",
   "metadata": {},
   "source": [
    "## 4. Initialize W&B Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4fc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B\n",
    "wandb.login()\n",
    "\n",
    "# Initialize W&B run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"grpo_imdb_gen{NUM_SAMPLE_GENERATIONS}_{timestamp}\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"grpo\",\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"model\": \"gpt2-large\",\n",
    "        \"dataset\": \"imdb\",\n",
    "        \"num_prompts\": len(grpo_dataset),\n",
    "        \"num_sample_generations\": NUM_SAMPLE_GENERATIONS,\n",
    "        \"use_reward_model\": USE_REWARD_MODEL,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"mini_batch_size\": MINI_BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"device\": str(device),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ W&B initialized: {run_name}\")\n",
    "print(f\"   Project: grpo\")\n",
    "print(f\"   Generations per prompt: {NUM_SAMPLE_GENERATIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f563662",
   "metadata": {},
   "source": [
    "## 5. Configure GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 3: Configuration du GRPO Trainer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# GRPO Configuration\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=GRPO_MODEL_PATH,\n",
    "    \n",
    "    # GRPO-specific\n",
    "    num_sample_generations=NUM_SAMPLE_GENERATIONS,\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Optimization\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.05,\n",
    "    \n",
    "    # Generation\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    do_sample=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Hardware optimization\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Other\n",
    "    remove_unused_columns=False,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ GRPO Config cr√©√©\")\n",
    "print(f\"\\nüìä Configuration:\")\n",
    "print(f\"   - Generations per prompt: {NUM_SAMPLE_GENERATIONS}\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   - Max new tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"   - Total prompts: {len(grpo_dataset)}\")\n",
    "print(f\"   - FP16: {grpo_config.fp16}\")\n",
    "print(f\"   - Gradient checkpointing: {grpo_config.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd068a",
   "metadata": {},
   "source": [
    "## 6. Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9850832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 4: D√©finition de la fonction de reward\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def compute_rewards(texts):\n",
    "    \"\"\"\n",
    "    Compute sentiment rewards for generated texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of generated texts (prompt + response)\n",
    "    \n",
    "    Returns:\n",
    "        rewards: List of reward scores (higher = more positive sentiment)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Truncate to max length for classifier\n",
    "        truncated = text[:512]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = reward_tokenizer(\n",
    "            truncated,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Compute reward\n",
    "        with torch.no_grad():\n",
    "            if USE_REWARD_MODEL:\n",
    "                # Trained reward model: scalar output\n",
    "                outputs = reward_model(**inputs)\n",
    "                reward = outputs.logits[0, 0].item()\n",
    "            else:\n",
    "                # Classifier: probability of POSITIVE class\n",
    "                outputs = reward_model(**inputs)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1)\n",
    "                reward = probs[0, 1].item()  # POSITIVE probability (0-1)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Test reward function\n",
    "print(\"\\nüß™ Test de la fonction de reward:\\n\")\n",
    "test_texts = [\n",
    "    \"This movie is absolutely amazing and wonderful!\",\n",
    "    \"This movie is terrible and boring.\"\n",
    "]\n",
    "\n",
    "test_rewards = compute_rewards(test_texts)\n",
    "for text, reward in zip(test_texts, test_rewards):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Reward: {reward:.4f}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Fonction de reward op√©rationnelle\")\n",
    "print(f\"   Mode: {'Reward Model' if USE_REWARD_MODEL else 'Classifier (siebert)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10de41a",
   "metadata": {},
   "source": [
    "## 7. Initialize GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 5: Initialisation du GRPO Trainer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize GRPO Trainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    args=grpo_config,\n",
    "    train_dataset=grpo_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    reward_function=compute_rewards,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ GRPO Trainer initialis√©\")\n",
    "print(f\"\\nüìä R√©sum√©:\")\n",
    "print(f\"   - Policy model: GPT-2-Large (SFT)\")\n",
    "print(f\"   - Reference model: GPT-2-Large (SFT frozen)\")\n",
    "print(f\"   - Reward: {'Trained model' if USE_REWARD_MODEL else 'Classifier direct'}\")\n",
    "print(f\"   - Prompts: {len(grpo_dataset)}\")\n",
    "print(f\"   - Generations/prompt: {NUM_SAMPLE_GENERATIONS}\")\n",
    "print(f\"   - Total generations: {len(grpo_dataset) * NUM_SAMPLE_GENERATIONS}\")\n",
    "print(f\"\\nüöÄ Pr√™t pour l'entra√Ænement GRPO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d516965",
   "metadata": {},
   "source": [
    "## 8. GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ed42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 6: Entra√Ænement GRPO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüöÄ D√©marrage de l'entra√Ænement GRPO...\")\n",
    "print(f\"   - {len(grpo_dataset)} prompts\")\n",
    "print(f\"   - {NUM_SAMPLE_GENERATIONS} g√©n√©rations par prompt\")\n",
    "print(f\"   - {NUM_EPOCHS} epoch(s)\\n\")\n",
    "\n",
    "# Clear GPU cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ GPU cache cleared\\n\")\n",
    "\n",
    "# Train\n",
    "grpo_trainer.train()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Entra√Ænement GRPO termin√©!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70753573",
   "metadata": {},
   "source": [
    "## 9. Save GRPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"√âTAPE 7: Sauvegarde du mod√®le GRPO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save model\n",
    "print(f\"\\nüíæ Sauvegarde du mod√®le GRPO dans: {GRPO_MODEL_PATH}\")\n",
    "policy_model.save_pretrained(GRPO_MODEL_PATH)\n",
    "tokenizer.save_pretrained(GRPO_MODEL_PATH)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le GRPO sauvegard√©!\")\n",
    "print(f\"\\nüìÅ Fichiers cr√©√©s:\")\n",
    "print(f\"   - {GRPO_MODEL_PATH}/pytorch_model.bin\")\n",
    "print(f\"   - {GRPO_MODEL_PATH}/config.json\")\n",
    "print(f\"   - {GRPO_MODEL_PATH}/tokenizer.json\")\n",
    "\n",
    "# Close W&B run\n",
    "wandb.finish()\n",
    "print(f\"\\n‚úÖ W&B run closed\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ GRPO TRAINING COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nüéØ Prochaines √©tapes:\")\n",
    "print(f\"   1. √âvaluer le mod√®le GRPO sur le test set\")\n",
    "print(f\"   2. Comparer avec SFT, DPO, PPO\")\n",
    "print(f\"   3. Tester d'autres valeurs de num_sample_generations (2, 8, 16)\")\n",
    "print(f\"   4. G√©n√©rer la courbe reward-KL (Figure 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cfb0ab",
   "metadata": {},
   "source": [
    "## 10. Test GRPO Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Test du mod√®le GRPO sur quelques exemples\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load GRPO model for testing\n",
    "test_model = AutoModelForCausalLM.from_pretrained(GRPO_MODEL_PATH)\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(GRPO_MODEL_PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    test_model = test_model.to(\"cuda\")\n",
    "\n",
    "test_model.eval()\n",
    "\n",
    "# Test on 5 random prompts from dataset\n",
    "import random\n",
    "random.seed(42)\n",
    "test_indices = random.sample(range(len(grpo_dataset)), 5)\n",
    "test_samples = grpo_dataset.select(test_indices)\n",
    "\n",
    "print(f\"\\nG√©n√©ration sur 5 prompts al√©atoires du dataset:\\n\")\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    prompt = sample[\"query\"]\n",
    "    \n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = test_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=40,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=test_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    continuation = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"\\nGRPO Generated: {continuation}\")\n",
    "    \n",
    "    # Compute reward\n",
    "    reward = compute_rewards([generated_text])[0]\n",
    "    print(f\"Reward: {reward:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚úÖ Test termin√©!\")\n",
    "print(f\"\\nüí° Le mod√®le GRPO devrait g√©n√©rer du texte avec un sentiment positif √©lev√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a0f24",
   "metadata": {},
   "source": [
    "## 11. Compare SFT vs GRPO (Side-by-side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Comparaison SFT vs GRPO sur les m√™mes prompts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load SFT model\n",
    "print(\"\\nüì• Chargement du mod√®le SFT...\")\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(SFT_MODEL_PATH)\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sft_model = sft_model.to(\"cuda\")\n",
    "\n",
    "sft_model.eval()\n",
    "print(\"‚úÖ Mod√®le SFT charg√©\")\n",
    "\n",
    "# Test on 3 prompts\n",
    "test_prompts = [\n",
    "    \"This movie is\",\n",
    "    \"I really enjoyed\",\n",
    "    \"The acting was\"\n",
    "]\n",
    "\n",
    "print(f\"\\nG√©n√©ration comparative sur {len(test_prompts)} prompts:\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # SFT generation\n",
    "    inputs_sft = sft_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs_sft = inputs_sft.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_sft = sft_model.generate(\n",
    "            **inputs_sft,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=sft_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    sft_text = sft_tokenizer.decode(outputs_sft[0], skip_special_tokens=True)\n",
    "    sft_continuation = sft_text[len(prompt):].strip()\n",
    "    sft_reward = compute_rewards([sft_text])[0]\n",
    "    \n",
    "    # GRPO generation\n",
    "    inputs_grpo = test_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs_grpo = inputs_grpo.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_grpo = test_model.generate(\n",
    "            **inputs_grpo,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=test_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    grpo_text = test_tokenizer.decode(outputs_grpo[0], skip_special_tokens=True)\n",
    "    grpo_continuation = grpo_text[len(prompt):].strip()\n",
    "    grpo_reward = compute_rewards([grpo_text])[0]\n",
    "    \n",
    "    # Display comparison\n",
    "    print(f\"\\nüìù SFT:   {sft_continuation}\")\n",
    "    print(f\"   Reward: {sft_reward:.4f}\")\n",
    "    print(f\"\\nüéØ GRPO:  {grpo_continuation}\")\n",
    "    print(f\"   Reward: {grpo_reward:.4f}\")\n",
    "    print(f\"\\nüìä Am√©lioration: {(grpo_reward - sft_reward):.4f} (+{((grpo_reward - sft_reward) / sft_reward * 100):.1f}%)\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚úÖ Comparaison termin√©e!\")\n",
    "print(f\"\\nüí° GRPO devrait g√©n√©rer du texte avec un reward plus √©lev√© que SFT\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
